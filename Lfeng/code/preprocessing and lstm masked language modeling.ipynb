{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e45b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/fengli/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/fengli/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/fengli/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/fengli/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in /Users/fengli/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad03035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 21:07:15.001215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /Users/fengli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fengli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c30d69",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "questions:\n",
    "- what to do with stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c181",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c08905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"THE EMPEROR'S NEW CLOTHES\\n\", '\\n', 'Many years ago, there was an Emperor, who was so excessively fond of\\n', 'new clothes, that he spent all his money in dress. He did not trouble\\n', 'himself in the least about his soldiers; nor did he care to go either to\\n', 'the theatre or the chase, except for the opportunities then afforded him\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/pg1597.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa27d9",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    data = \"\".join(line)\n",
    "    data = word_tokenize(data)\n",
    "    words = \" \".join(data)\n",
    "    lower_w = words.lower()\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = lower_w.split(\" \")\n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    output = []\n",
    "    for word in filtered_sentence:\n",
    "        output.append((ps.stem(word)))\n",
    "    final_out = \" \".join(output)\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86961d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f35c73d4",
   "metadata": {},
   "source": [
    "### get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39342b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    data = data.replace('\\n', ' ').split(' ')\n",
    "    words = list(set(data))\n",
    "    vocabulary = {word:index for index, word in enumerate(words)}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    return vocabulary, vocab_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(lines)\n",
    "vocabulary, vocab_size = get_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021aeda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe02bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fell': 0,\n",
       " 'queri': 1,\n",
       " 'rub': 2,\n",
       " 'silken': 3,\n",
       " 'soon': 4,\n",
       " 'mead': 5,\n",
       " 'charm': 6,\n",
       " 'restless': 7,\n",
       " 'soldier': 8,\n",
       " 'well-cram': 9,\n",
       " 'sunni': 10,\n",
       " 'sprang': 11,\n",
       " 'string': 12,\n",
       " 'others.': 13,\n",
       " 'rosy-cheek': 14,\n",
       " 'stamina': 15,\n",
       " 'execution.': 16,\n",
       " 'carriag': 17,\n",
       " 'nothing.': 18,\n",
       " 'circumst': 19,\n",
       " 'desk': 20,\n",
       " 'someon': 21,\n",
       " 'nought': 22,\n",
       " 'many-color': 23,\n",
       " 'matter': 24,\n",
       " 'gate': 25,\n",
       " 'pursu': 26,\n",
       " 'fork': 27,\n",
       " 'ruin': 28,\n",
       " 'citi': 29,\n",
       " 'auction': 30,\n",
       " 'burnt-out': 31,\n",
       " 'hungri': 32,\n",
       " 'palac': 33,\n",
       " 'grapes.': 34,\n",
       " 'viz': 35,\n",
       " 'extent': 36,\n",
       " 'vice': 37,\n",
       " 'else.': 38,\n",
       " 'honey-be': 39,\n",
       " 'courteous': 40,\n",
       " 'nichola': 41,\n",
       " 'scarf': 42,\n",
       " 'prescrib': 43,\n",
       " 'ceil': 44,\n",
       " 'pave': 45,\n",
       " 'nest': 46,\n",
       " 'bur': 47,\n",
       " 'bell': 48,\n",
       " 'again.': 49,\n",
       " 'harm': 50,\n",
       " 'pet': 51,\n",
       " 'summer-tim': 52,\n",
       " 'mistress': 53,\n",
       " 'form': 54,\n",
       " 'revel': 55,\n",
       " 'bride': 56,\n",
       " 'sink': 57,\n",
       " 'perceiv': 58,\n",
       " '!': 59,\n",
       " 'truli': 60,\n",
       " 'christianshafen.': 61,\n",
       " 'councillor': 62,\n",
       " 'second': 63,\n",
       " 'mr.': 64,\n",
       " 'oak-tre': 65,\n",
       " 'portrait': 66,\n",
       " 'perpendicularli': 67,\n",
       " 'bring': 68,\n",
       " 'rang': 69,\n",
       " 'safe': 70,\n",
       " 'emperor': 71,\n",
       " 'prasto': 72,\n",
       " 'narrat': 73,\n",
       " 'stupid': 74,\n",
       " 'grew': 75,\n",
       " 'noth': 76,\n",
       " 'clump': 77,\n",
       " 'creeping-pl': 78,\n",
       " 'forthwith': 79,\n",
       " 'shalt': 80,\n",
       " 'purest': 81,\n",
       " 'known': 82,\n",
       " 'bed-rid': 83,\n",
       " 'result': 84,\n",
       " 'fold': 85,\n",
       " 'pale': 86,\n",
       " 'flower-garden': 87,\n",
       " 'properti': 88,\n",
       " 'train': 89,\n",
       " 'town': 90,\n",
       " 'snowstorm': 91,\n",
       " 'chanc': 92,\n",
       " 'introduct': 93,\n",
       " 'drip': 94,\n",
       " 'alm': 95,\n",
       " 'bestir': 96,\n",
       " 'delay': 97,\n",
       " 'undress': 98,\n",
       " 'almond-tre': 99,\n",
       " 'complic': 100,\n",
       " 'loom': 101,\n",
       " 'skeleton': 102,\n",
       " 'sixteen': 103,\n",
       " 'leav': 104,\n",
       " 'coverlet': 105,\n",
       " 'paid': 106,\n",
       " 'eve': 107,\n",
       " 'summer': 108,\n",
       " 'starlit': 109,\n",
       " 'cloth.': 110,\n",
       " \"say'st\": 111,\n",
       " 'mile-broad': 112,\n",
       " 'incens': 113,\n",
       " 'mingl': 114,\n",
       " 'strengthen': 115,\n",
       " 'wrote': 116,\n",
       " 'fatigu': 117,\n",
       " 'swarm': 118,\n",
       " 'embroid': 119,\n",
       " 'men': 120,\n",
       " 'corso': 121,\n",
       " 'particularli': 122,\n",
       " 'courtier': 123,\n",
       " 'trembl': 124,\n",
       " 'times.': 125,\n",
       " 'fifty-two': 126,\n",
       " 'sun-ray': 127,\n",
       " 'struggl': 128,\n",
       " 'miracl': 129,\n",
       " 'motionless': 130,\n",
       " 'moss': 131,\n",
       " 'love': 132,\n",
       " 'unhind': 133,\n",
       " 'pseudo-herschel': 134,\n",
       " 'boil': 135,\n",
       " 'visitor': 136,\n",
       " 'visit': 137,\n",
       " 'flap': 138,\n",
       " 'enter.': 139,\n",
       " 'later': 140,\n",
       " 'spread': 141,\n",
       " 'farewel': 142,\n",
       " 'secur': 143,\n",
       " 'sing': 144,\n",
       " 'fortun': 145,\n",
       " 'mane': 146,\n",
       " 'hold': 147,\n",
       " 'villag': 148,\n",
       " 'even': 149,\n",
       " 'embodi': 150,\n",
       " 'pipe': 151,\n",
       " 'modest': 152,\n",
       " 'neighborhood': 153,\n",
       " 'court.': 154,\n",
       " 'discov': 155,\n",
       " 'shame': 156,\n",
       " 'rich': 157,\n",
       " 'plumtre': 158,\n",
       " 'spear': 159,\n",
       " 'nightingal': 160,\n",
       " 'profus': 161,\n",
       " 'use': 162,\n",
       " 'countri': 163,\n",
       " 'hero': 164,\n",
       " 'languag': 165,\n",
       " 'desk.': 166,\n",
       " 'geography-book': 167,\n",
       " 'tore': 168,\n",
       " 'flask': 169,\n",
       " 'ball-dress': 170,\n",
       " 'went': 171,\n",
       " 'finland': 172,\n",
       " 'breakneck': 173,\n",
       " 'pigsti': 174,\n",
       " 'long': 175,\n",
       " 'butter': 176,\n",
       " 'destroy.': 177,\n",
       " 'veloc': 178,\n",
       " 'seem': 179,\n",
       " 'marbl': 180,\n",
       " 'wept': 181,\n",
       " 'outward': 182,\n",
       " 'practis': 183,\n",
       " 'spite': 184,\n",
       " 'iv': 185,\n",
       " 'tendril': 186,\n",
       " 'creep': 187,\n",
       " 'gentl': 188,\n",
       " 'churchyard': 189,\n",
       " 'gaili': 190,\n",
       " 'vigor': 191,\n",
       " 'filthiest': 192,\n",
       " 'rim': 193,\n",
       " 'arabesqu': 194,\n",
       " 'swoon': 195,\n",
       " 'bold': 196,\n",
       " 'fare': 197,\n",
       " 'pack': 198,\n",
       " 'tranc': 199,\n",
       " 'leave.': 200,\n",
       " 'waggon': 201,\n",
       " 'awaken': 202,\n",
       " 'grief': 203,\n",
       " \"o'er\": 204,\n",
       " 'crimin': 205,\n",
       " 'princess.': 206,\n",
       " \"snail'\": 207,\n",
       " 'distanc': 208,\n",
       " '.': 209,\n",
       " 'weaver': 210,\n",
       " 'ran': 211,\n",
       " 'sir': 212,\n",
       " 'hook': 213,\n",
       " 'weather-cock': 214,\n",
       " 'ever-lov': 215,\n",
       " 'circul': 216,\n",
       " 'forget': 217,\n",
       " 'discours': 218,\n",
       " 'groan': 219,\n",
       " 'bade': 220,\n",
       " 'sole': 221,\n",
       " 'recal': 222,\n",
       " 'oil-skin': 223,\n",
       " 'suffoc': 224,\n",
       " 'run': 225,\n",
       " 'struck': 226,\n",
       " 'fortune-tel': 227,\n",
       " 'crush': 228,\n",
       " 'hous': 229,\n",
       " 'alon': 230,\n",
       " 'ceremoni': 231,\n",
       " 'perspir': 232,\n",
       " 'apple-tre': 233,\n",
       " 'misfortun': 234,\n",
       " 'unload': 235,\n",
       " 'grandmoth': 236,\n",
       " 'displeas': 237,\n",
       " 'well-mann': 238,\n",
       " 'dong': 239,\n",
       " 'although': 240,\n",
       " 'pleas': 241,\n",
       " 'escap': 242,\n",
       " 'preced': 243,\n",
       " 'thenc': 244,\n",
       " 'radiant': 245,\n",
       " 'water-pl': 246,\n",
       " 'occup': 247,\n",
       " 'cupola': 248,\n",
       " 'satisfi': 249,\n",
       " 'dispel': 250,\n",
       " 'well-light': 251,\n",
       " 'midshipman': 252,\n",
       " 'to-day': 253,\n",
       " 'honestli': 254,\n",
       " 'councillorship': 255,\n",
       " 'whistl': 256,\n",
       " 'tremend': 257,\n",
       " 'cross-bow': 258,\n",
       " 'fled': 259,\n",
       " 'scrape': 260,\n",
       " 'thrash': 261,\n",
       " 'spici': 262,\n",
       " 'feel': 263,\n",
       " 'furthermor': 264,\n",
       " 'better': 265,\n",
       " 'heed': 266,\n",
       " 'cake': 267,\n",
       " 'cherry-orchard': 268,\n",
       " 'moonshin': 269,\n",
       " 'becom': 270,\n",
       " 'chase': 271,\n",
       " 'chest': 272,\n",
       " 'sail': 273,\n",
       " 'approbation.': 274,\n",
       " 'quay': 275,\n",
       " 'border': 276,\n",
       " 'farthest': 277,\n",
       " 'grown-up-person': 278,\n",
       " 'horse-cloth': 279,\n",
       " 'immedi': 280,\n",
       " 'doubtless': 281,\n",
       " 'freezingli': 282,\n",
       " 'treatment': 283,\n",
       " 'shut': 284,\n",
       " 'abash': 285,\n",
       " 'colder': 286,\n",
       " 'fish-skin': 287,\n",
       " 'ingemann': 288,\n",
       " 'rail': 289,\n",
       " 'commonplac': 290,\n",
       " 'fill': 291,\n",
       " 'spruce': 292,\n",
       " 'diet': 293,\n",
       " 'straight': 294,\n",
       " 'lop': 295,\n",
       " 'soup': 296,\n",
       " 'boreali': 297,\n",
       " 'lone': 298,\n",
       " 'fain': 299,\n",
       " 'bird': 300,\n",
       " 'heavi': 301,\n",
       " 'hear': 302,\n",
       " 'dweller': 303,\n",
       " 'asham': 304,\n",
       " 'wet': 305,\n",
       " 'lion': 306,\n",
       " 'intox': 307,\n",
       " 'dr.': 308,\n",
       " 'roses.': 309,\n",
       " 'midst': 310,\n",
       " 'kind-heart': 311,\n",
       " 'wield': 312,\n",
       " 'ventur': 313,\n",
       " 'benefici': 314,\n",
       " 'funer': 315,\n",
       " 'elast': 316,\n",
       " 'alp': 317,\n",
       " 'serv': 318,\n",
       " 'restor': 319,\n",
       " 'shape': 320,\n",
       " 'unpleas': 321,\n",
       " 'state': 322,\n",
       " 'expens': 323,\n",
       " 'curtain': 324,\n",
       " 'remain.': 325,\n",
       " 'intim': 326,\n",
       " 'author': 327,\n",
       " 'pension.': 328,\n",
       " 'bodili': 329,\n",
       " 'foremost': 330,\n",
       " 'adventur': 331,\n",
       " 'porcelain': 332,\n",
       " 'waterpail': 333,\n",
       " 'upper': 334,\n",
       " 'fun': 335,\n",
       " 'wed': 336,\n",
       " 'sympathi': 337,\n",
       " 'fraction': 338,\n",
       " 'farm-hous': 339,\n",
       " 'gabl': 340,\n",
       " 'strictli': 341,\n",
       " 'laudatori': 342,\n",
       " 'hog': 343,\n",
       " 'accord': 344,\n",
       " 'fowl': 345,\n",
       " 'great': 346,\n",
       " 'wad': 347,\n",
       " 'imbib': 348,\n",
       " 'high': 349,\n",
       " 'ignor': 350,\n",
       " 'mysteri': 351,\n",
       " 'unfit': 352,\n",
       " 'steam-vessel': 353,\n",
       " \"author'\": 354,\n",
       " 'passag': 355,\n",
       " 'cask': 356,\n",
       " 'practic': 357,\n",
       " 'sundri': 358,\n",
       " 'apothegm': 359,\n",
       " 'shadow': 360,\n",
       " 'stand': 361,\n",
       " 'door': 362,\n",
       " 'conclud': 363,\n",
       " 'clean': 364,\n",
       " 'personag': 365,\n",
       " 'tomb': 366,\n",
       " 'leggin': 367,\n",
       " 'tipto': 368,\n",
       " 'lanc': 369,\n",
       " 'coffee-tre': 370,\n",
       " 'mountain': 371,\n",
       " 'easy-chair': 372,\n",
       " 'appleblossom': 373,\n",
       " 'recover.': 374,\n",
       " 'suitabl': 375,\n",
       " 'enthusiast': 376,\n",
       " 'rascal': 377,\n",
       " 'cure': 378,\n",
       " 'good-look': 379,\n",
       " 'enabl': 380,\n",
       " 'to-night': 381,\n",
       " 'soro': 382,\n",
       " 'everyon': 383,\n",
       " 'aloud': 384,\n",
       " 'fate': 385,\n",
       " 'carpet': 386,\n",
       " 'salad': 387,\n",
       " 'marriag': 388,\n",
       " 'lowest': 389,\n",
       " 'retain': 390,\n",
       " 'soothsay': 391,\n",
       " 'ball-room': 392,\n",
       " 'bedroom': 393,\n",
       " 'behav': 394,\n",
       " 'pronoun': 395,\n",
       " 'needl': 396,\n",
       " 'mind': 397,\n",
       " 'ill': 398,\n",
       " 'nap': 399,\n",
       " 'nay': 400,\n",
       " 'amidst': 401,\n",
       " 'report': 402,\n",
       " 'diminut': 403,\n",
       " 'loop': 404,\n",
       " 'plush': 405,\n",
       " 'respons': 406,\n",
       " 'tea-parti': 407,\n",
       " 'millin': 408,\n",
       " 'mistak': 409,\n",
       " 'anxieti': 410,\n",
       " '2000': 411,\n",
       " 'open': 412,\n",
       " 'teach': 413,\n",
       " 'decid': 414,\n",
       " 'quiet': 415,\n",
       " 'hoars': 416,\n",
       " 'pleasant': 417,\n",
       " 'setting.': 418,\n",
       " 'plain': 419,\n",
       " 'whisper': 420,\n",
       " 'dinner': 421,\n",
       " 'pierc': 422,\n",
       " 'spent': 423,\n",
       " 'larg': 424,\n",
       " 'condit': 425,\n",
       " 'half': 426,\n",
       " '1807': 427,\n",
       " 'asleep': 428,\n",
       " 'amid': 429,\n",
       " 'harmless': 430,\n",
       " 'plum-tre': 431,\n",
       " 'impati': 432,\n",
       " 'box': 433,\n",
       " 'willingli': 434,\n",
       " 'rattl': 435,\n",
       " 'behold': 436,\n",
       " 'taught': 437,\n",
       " 'melt.': 438,\n",
       " 'raw': 439,\n",
       " 'add': 440,\n",
       " 'loav': 441,\n",
       " 'vanish': 442,\n",
       " 'rustl': 443,\n",
       " 'portal': 444,\n",
       " 'tea': 445,\n",
       " 'send': 446,\n",
       " 'possibl': 447,\n",
       " 'weavers.': 448,\n",
       " 'everyth': 449,\n",
       " 'favorit': 450,\n",
       " 'stream': 451,\n",
       " 'famili': 452,\n",
       " 'shorter': 453,\n",
       " 'butcher': 454,\n",
       " 'seemingli': 455,\n",
       " 'bid': 456,\n",
       " 'marvel': 457,\n",
       " 'joy': 458,\n",
       " 'unimagin': 459,\n",
       " 'threateningli': 460,\n",
       " 'defeat': 461,\n",
       " 'narrow': 462,\n",
       " \"'to\": 463,\n",
       " 'garret': 464,\n",
       " 'abund': 465,\n",
       " 'someth': 466,\n",
       " 'floor': 467,\n",
       " 'sound': 468,\n",
       " 'sew': 469,\n",
       " 'percept': 470,\n",
       " 'stone': 471,\n",
       " 'drew': 472,\n",
       " 'police-offic': 473,\n",
       " 'opera': 474,\n",
       " 'suppos': 475,\n",
       " 'vain': 476,\n",
       " 'intend': 477,\n",
       " 'schoolmast': 478,\n",
       " 'proverb': 479,\n",
       " 'nybod': 480,\n",
       " 'audience.': 481,\n",
       " 'du': 482,\n",
       " 'sleep': 483,\n",
       " 'otherwis': 484,\n",
       " 'period': 485,\n",
       " 'press': 486,\n",
       " 'silent': 487,\n",
       " 'loui': 488,\n",
       " 'woo': 489,\n",
       " 'ass': 490,\n",
       " 'sword': 491,\n",
       " 'tire': 492,\n",
       " \"gentlemen'\": 493,\n",
       " 'ten': 494,\n",
       " 'appoint': 495,\n",
       " 'fragranc': 496,\n",
       " 'two.': 497,\n",
       " 'gust': 498,\n",
       " 'haycock': 499,\n",
       " 'prefer': 500,\n",
       " 'name': 501,\n",
       " 'declar': 502,\n",
       " 'polic': 503,\n",
       " 'everyday': 504,\n",
       " 'landscap': 505,\n",
       " 'french': 506,\n",
       " 'speed': 507,\n",
       " 'slope': 508,\n",
       " 'merrili': 509,\n",
       " 'everything.': 510,\n",
       " 'dwelt': 511,\n",
       " 'wander': 512,\n",
       " 'owner': 513,\n",
       " 'wretch': 514,\n",
       " 'sky': 515,\n",
       " 'crippled-beggar': 516,\n",
       " 'sat': 517,\n",
       " 'faculti': 518,\n",
       " 'sleek': 519,\n",
       " 'leathern': 520,\n",
       " 'weed': 521,\n",
       " 'market-plac': 522,\n",
       " 'clamber': 523,\n",
       " 'parti': 524,\n",
       " 'spinning-wheel': 525,\n",
       " 'narcissu': 526,\n",
       " 'valley': 527,\n",
       " 'riddl': 528,\n",
       " 'gother': 529,\n",
       " 'bet': 530,\n",
       " 'karen': 531,\n",
       " 'herostrat': 532,\n",
       " 'proud': 533,\n",
       " 'theologian': 534,\n",
       " 'occur': 535,\n",
       " 'bottom': 536,\n",
       " 'side': 537,\n",
       " 'came': 538,\n",
       " 'shelter': 539,\n",
       " 'washerwoman': 540,\n",
       " 'lighten': 541,\n",
       " 'still': 542,\n",
       " 'event': 543,\n",
       " 'playfellow': 544,\n",
       " 'jam': 545,\n",
       " 'met': 546,\n",
       " 'discoveri': 547,\n",
       " 'flower': 548,\n",
       " 'have.': 549,\n",
       " 'hill-sid': 550,\n",
       " 'errand': 551,\n",
       " \"'do\": 552,\n",
       " 'conceit': 553,\n",
       " 'flaminiu': 554,\n",
       " 'generos': 555,\n",
       " 'balustrad': 556,\n",
       " 'hearth': 557,\n",
       " 'firm': 558,\n",
       " 'speech': 559,\n",
       " 'snatch': 560,\n",
       " 'dri': 561,\n",
       " 'moon': 562,\n",
       " 'bachelor': 563,\n",
       " 'ditch': 564,\n",
       " 'forget.': 565,\n",
       " 'pretend': 566,\n",
       " 'posit': 567,\n",
       " 'country-seat': 568,\n",
       " 'imposs': 569,\n",
       " 'audienc': 570,\n",
       " \"gentleman'\": 571,\n",
       " 'vanquish': 572,\n",
       " 'stunt': 573,\n",
       " 'nowher': 574,\n",
       " 'stuf': 575,\n",
       " 'furnish': 576,\n",
       " 'weg': 577,\n",
       " 'aurora': 578,\n",
       " 'joy.': 579,\n",
       " 'shout': 580,\n",
       " 'street': 581,\n",
       " 'daub': 582,\n",
       " 'disagre': 583,\n",
       " 'innumer': 584,\n",
       " 'habro-platz': 585,\n",
       " 'reproach': 586,\n",
       " 'summer-day': 587,\n",
       " 'figur': 588,\n",
       " 'bubbl': 589,\n",
       " 'nut-kernel': 590,\n",
       " 'listening.': 591,\n",
       " 'women': 592,\n",
       " 'well-dress': 593,\n",
       " 'tini': 594,\n",
       " 'howev': 595,\n",
       " 'wearer': 596,\n",
       " 'farth': 597,\n",
       " 'flower-box': 598,\n",
       " 'exert': 599,\n",
       " 'profit': 600,\n",
       " 'russian': 601,\n",
       " 'whine': 602,\n",
       " 'inexhaust': 603,\n",
       " 'stale': 604,\n",
       " 'shopman': 605,\n",
       " 'stay': 606,\n",
       " 'sun-light': 607,\n",
       " 'lies.': 608,\n",
       " 'father': 609,\n",
       " 'perfectli': 610,\n",
       " 'tent': 611,\n",
       " 'ha': 612,\n",
       " 'degre': 613,\n",
       " 'ordain': 614,\n",
       " 'titl': 615,\n",
       " 'princess': 616,\n",
       " 'resolut': 617,\n",
       " 'influenc': 618,\n",
       " 'glass-cas': 619,\n",
       " 'crew': 620,\n",
       " 'lack': 621,\n",
       " 'simpleton': 622,\n",
       " 'reign': 623,\n",
       " 'bower': 624,\n",
       " 'poesi': 625,\n",
       " 'ambassador': 626,\n",
       " 'far': 627,\n",
       " 'whether': 628,\n",
       " 'dryad': 629,\n",
       " 'waiting-maid': 630,\n",
       " 'nice': 631,\n",
       " \"'dryad\": 632,\n",
       " 'damsel': 633,\n",
       " 'hurt': 634,\n",
       " 'content': 635,\n",
       " 'botanist': 636,\n",
       " \"'ti\": 637,\n",
       " 'rain-drop': 638,\n",
       " 'tune': 639,\n",
       " 'prepar': 640,\n",
       " 'bornholmish': 641,\n",
       " 'similar': 642,\n",
       " 'rancid': 643,\n",
       " 'handl': 644,\n",
       " 'sculptor': 645,\n",
       " 'rose-color': 646,\n",
       " 'gibberish': 647,\n",
       " 'mari': 648,\n",
       " 'worst': 649,\n",
       " 'grain': 650,\n",
       " 'beat': 651,\n",
       " 'wore': 652,\n",
       " 'govern': 653,\n",
       " 'dishearten': 654,\n",
       " 'burial-vault': 655,\n",
       " 'plant': 656,\n",
       " 'weight': 657,\n",
       " 'encourag': 658,\n",
       " 'angrili': 659,\n",
       " 'frederick': 660,\n",
       " 'ladder': 661,\n",
       " 'kickery-ki': 662,\n",
       " 'awak': 663,\n",
       " 'caught': 664,\n",
       " 'fasten': 665,\n",
       " 'richli': 666,\n",
       " 'lyric': 667,\n",
       " 'permiss': 668,\n",
       " 'methink': 669,\n",
       " 'astound': 670,\n",
       " 'robe': 671,\n",
       " 'tansi': 672,\n",
       " 'coo': 673,\n",
       " 'tack': 674,\n",
       " 'pine-forest': 675,\n",
       " 'poor': 676,\n",
       " 'daughter.': 677,\n",
       " 'mummeri': 678,\n",
       " 'anecdot': 679,\n",
       " 'splendidly-dress': 680,\n",
       " 'metamorphosi': 681,\n",
       " 'deck': 682,\n",
       " 'fruit': 683,\n",
       " 'await': 684,\n",
       " 'pillar': 685,\n",
       " 'discreet': 686,\n",
       " 'clever': 687,\n",
       " 'veget': 688,\n",
       " 'thickest': 689,\n",
       " 'sup': 690,\n",
       " 'expect': 691,\n",
       " 'cradl': 692,\n",
       " 'kind': 693,\n",
       " 'witti': 694,\n",
       " 'spout': 695,\n",
       " 'space': 696,\n",
       " 'teas': 697,\n",
       " 'much': 698,\n",
       " 'well-known': 699,\n",
       " 'wit': 700,\n",
       " 'weather': 701,\n",
       " 'pussy-cat': 702,\n",
       " 'piou': 703,\n",
       " 'paroquet': 704,\n",
       " 'unobserv': 705,\n",
       " 'dissatisfact': 706,\n",
       " 'heathen': 707,\n",
       " 'vista': 708,\n",
       " 'nicest': 709,\n",
       " 'relat': 710,\n",
       " 'sang': 711,\n",
       " 'emissari': 712,\n",
       " 'snowdrop': 713,\n",
       " 'viewing-room': 714,\n",
       " 'thine': 715,\n",
       " 'rosenburg': 716,\n",
       " 'commemor': 717,\n",
       " 'italia.': 718,\n",
       " 'side-door': 719,\n",
       " 'skates.': 720,\n",
       " 'starch': 721,\n",
       " 'varnish': 722,\n",
       " 'particular': 723,\n",
       " 'perish': 724,\n",
       " 'overstrain': 725,\n",
       " 'europ': 726,\n",
       " 'flush': 727,\n",
       " 'boy': 728,\n",
       " 'repent': 729,\n",
       " 'difficult': 730,\n",
       " 'satisfact': 731,\n",
       " 'a.d.': 732,\n",
       " 'violin': 733,\n",
       " 'honor': 734,\n",
       " \"aunt'\": 735,\n",
       " 'hauch': 736,\n",
       " 'anim': 737,\n",
       " 'benign': 738,\n",
       " 'gracious': 739,\n",
       " 'rischt': 740,\n",
       " 'simplest': 741,\n",
       " 'sought': 742,\n",
       " 'proper': 743,\n",
       " 'boldli': 744,\n",
       " 'either': 745,\n",
       " 'festiv': 746,\n",
       " 'cobbler': 747,\n",
       " 'widow': 748,\n",
       " 'abod': 749,\n",
       " 'is.': 750,\n",
       " 'term': 751,\n",
       " 'famou': 752,\n",
       " 'regard': 753,\n",
       " 'toward': 754,\n",
       " 'teapot': 755,\n",
       " 'tempest': 756,\n",
       " 'dragon': 757,\n",
       " 'stoop': 758,\n",
       " 'bread-fruit': 759,\n",
       " 'sloth': 760,\n",
       " 'bull-dog': 761,\n",
       " 'elder': 762,\n",
       " 'mark': 763,\n",
       " 'syllabl': 764,\n",
       " 'to.': 765,\n",
       " 'complexion': 766,\n",
       " 'husband': 767,\n",
       " 'hand': 768,\n",
       " 'talk': 769,\n",
       " 'guess': 770,\n",
       " 'peoni': 771,\n",
       " 'show': 772,\n",
       " 'lectur': 773,\n",
       " 'though': 774,\n",
       " 'pail': 775,\n",
       " 'skin': 776,\n",
       " 'mace': 777,\n",
       " 'splendor': 778,\n",
       " 'thyme': 779,\n",
       " 'symbol': 780,\n",
       " 'easili': 781,\n",
       " 'detail': 782,\n",
       " 'reclus': 783,\n",
       " 'armor': 784,\n",
       " 'advis': 785,\n",
       " 'grandchildren': 786,\n",
       " 'rightli': 787,\n",
       " 'doorway.': 788,\n",
       " 'gladli': 789,\n",
       " 'trumpet': 790,\n",
       " 'germin': 791,\n",
       " 'bump': 792,\n",
       " 'laps': 793,\n",
       " 'allud': 794,\n",
       " 'muff': 795,\n",
       " 'barren': 796,\n",
       " 'docendi': 797,\n",
       " 'scream': 798,\n",
       " 'dishonor': 799,\n",
       " 'borrow': 800,\n",
       " 'tinkl': 801,\n",
       " 'endeavor': 802,\n",
       " 'freez': 803,\n",
       " 'moveth': 804,\n",
       " 'bearer': 805,\n",
       " 'rest': 806,\n",
       " 'crowd': 807,\n",
       " \"hog'\": 808,\n",
       " 'kill': 809,\n",
       " 'broad-brim': 810,\n",
       " 'hung': 811,\n",
       " 'axe': 812,\n",
       " 'number': 813,\n",
       " 'row': 814,\n",
       " 'perch': 815,\n",
       " 'dreari': 816,\n",
       " 'bridg': 817,\n",
       " 'egypt': 818,\n",
       " 'bit': 819,\n",
       " 'gloom': 820,\n",
       " 'prophet': 821,\n",
       " 'immediately.': 822,\n",
       " 'shoemak': 823,\n",
       " 'confer': 824,\n",
       " 'heath': 825,\n",
       " 'card-tabl': 826,\n",
       " 'knapsack': 827,\n",
       " 'way': 828,\n",
       " 'imprint': 829,\n",
       " 'individu': 830,\n",
       " 'imperish': 831,\n",
       " 'rubbish': 832,\n",
       " 'heap': 833,\n",
       " 'nettl': 834,\n",
       " 'contrari': 835,\n",
       " 'appar': 836,\n",
       " 'clothes-press': 837,\n",
       " 'action': 838,\n",
       " 'electricity.': 839,\n",
       " 'reduc': 840,\n",
       " 'smile': 841,\n",
       " 'memori': 842,\n",
       " \"i'll\": 843,\n",
       " 'occasion': 844,\n",
       " 'church-tow': 845,\n",
       " 'besid': 846,\n",
       " 'plantain': 847,\n",
       " 'cupid': 848,\n",
       " 'attun': 849,\n",
       " 'innoc': 850,\n",
       " 'every-day': 851,\n",
       " 'duu': 852,\n",
       " 'ride': 853,\n",
       " 'variou': 854,\n",
       " 'rosetre': 855,\n",
       " 'charact': 856,\n",
       " 'pretti': 857,\n",
       " 'cleverest': 858,\n",
       " 'debt.': 859,\n",
       " 'mantl': 860,\n",
       " 'graciou': 861,\n",
       " 'schoolboy': 862,\n",
       " 'materi': 863,\n",
       " 'swineherd.': 864,\n",
       " 'instantan': 865,\n",
       " 'knew': 866,\n",
       " 'lustr': 867,\n",
       " 'footmen': 868,\n",
       " 'matters.': 869,\n",
       " 'order': 870,\n",
       " 'repair': 871,\n",
       " 'phantasmagoria': 872,\n",
       " 'contend': 873,\n",
       " 'nail': 874,\n",
       " 'lightn': 875,\n",
       " 'hum': 876,\n",
       " 'chamberlain': 877,\n",
       " 'creak': 878,\n",
       " 'patienc': 879,\n",
       " 'poorest': 880,\n",
       " 'qualiti': 881,\n",
       " 'slept': 882,\n",
       " 'church-bel': 883,\n",
       " 'pea': 884,\n",
       " 'afraid.': 885,\n",
       " 'fro': 886,\n",
       " 'a.': 887,\n",
       " 'judgment': 888,\n",
       " 'trunk': 889,\n",
       " 'stagnant': 890,\n",
       " 'lackey': 891,\n",
       " 'reckon': 892,\n",
       " 'quit': 893,\n",
       " 'temper': 894,\n",
       " 'hem': 895,\n",
       " 'sort': 896,\n",
       " 'brought': 897,\n",
       " 'inn': 898,\n",
       " 'strong-stem': 899,\n",
       " 'stiff': 900,\n",
       " 'tri': 901,\n",
       " 'unwont': 902,\n",
       " 'satir': 903,\n",
       " 'toll': 904,\n",
       " 'kitchen-pot': 905,\n",
       " 'lap': 906,\n",
       " 'bye': 907,\n",
       " 'mean': 908,\n",
       " 'trial': 909,\n",
       " 'twig': 910,\n",
       " 'spare': 911,\n",
       " 'bath': 912,\n",
       " 'darkest': 913,\n",
       " 'out-of-door': 914,\n",
       " 'entir': 915,\n",
       " 'stair': 916,\n",
       " 'progeni': 917,\n",
       " 'sank': 918,\n",
       " 'brain': 919,\n",
       " 'rebelli': 920,\n",
       " 'ceas': 921,\n",
       " 'pace': 922,\n",
       " 'ay': 923,\n",
       " 'glass.': 924,\n",
       " 'longer.': 925,\n",
       " 'wo': 926,\n",
       " 'legion': 927,\n",
       " 'honest': 928,\n",
       " 'bewild': 929,\n",
       " 'reveal': 930,\n",
       " 'care': 931,\n",
       " 'scuffl': 932,\n",
       " 'sweetheart': 933,\n",
       " 'benefit': 934,\n",
       " 'mankind': 935,\n",
       " 'cottag': 936,\n",
       " 'shadow-skul': 937,\n",
       " 'rogu': 938,\n",
       " 'clung': 939,\n",
       " 'rank': 940,\n",
       " 'smelt': 941,\n",
       " 'assuredli': 942,\n",
       " 'unbecom': 943,\n",
       " 'backward': 944,\n",
       " 'snail': 945,\n",
       " 'pole': 946,\n",
       " 'insinu': 947,\n",
       " 'reach': 948,\n",
       " 'chastis': 949,\n",
       " 'ricketi': 950,\n",
       " 'house-dog': 951,\n",
       " 'ranunculu': 952,\n",
       " 'flower-pot': 953,\n",
       " 'enlist': 954,\n",
       " 'shudder': 955,\n",
       " 'chirp': 956,\n",
       " 'bank.': 957,\n",
       " 'stuff.': 958,\n",
       " 'hue': 959,\n",
       " 'bacon': 960,\n",
       " 'perform': 961,\n",
       " 'partner': 962,\n",
       " 'twice': 963,\n",
       " 'pardon': 964,\n",
       " 'magician': 965,\n",
       " 'plump': 966,\n",
       " 'mother': 967,\n",
       " 'dim': 968,\n",
       " 'affirm': 969,\n",
       " 'downward': 970,\n",
       " 'adorn': 971,\n",
       " 'glow': 972,\n",
       " 'small': 973,\n",
       " 'nose': 974,\n",
       " 'fatal': 975,\n",
       " 'canal': 976,\n",
       " 'ill.': 977,\n",
       " 'produc': 978,\n",
       " 'whirlwind': 979,\n",
       " 'saw': 980,\n",
       " 'manner': 981,\n",
       " 'calcul': 982,\n",
       " 'understand': 983,\n",
       " 'connect': 984,\n",
       " 'lamb': 985,\n",
       " 'sheer': 986,\n",
       " 'asid': 987,\n",
       " 'insati': 988,\n",
       " 'devoutli': 989,\n",
       " 'cost': 990,\n",
       " 'impos': 991,\n",
       " 'beggar': 992,\n",
       " 'demand': 993,\n",
       " 'nanni': 994,\n",
       " 'shoe': 995,\n",
       " 'crack': 996,\n",
       " 'intent': 997,\n",
       " 'belong': 998,\n",
       " 'fish': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355a1a",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM Masked Language Modeling\n",
    "\n",
    "references: \n",
    "\n",
    "https://keras.io/examples/nlp/masked_language_modeling/#create-bert-model-pretraining-model-for-masked-language-modeling\n",
    "\n",
    "https://www.kaggle.com/code/ritvik1909/masked-language-modelling-rnn#Data-Preparation\n",
    "\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "questions:\n",
    "- should we split data by sentence instead of by fixed window size of 20?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51007c",
   "metadata": {},
   "source": [
    "### more data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5b6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "vocabulary['[mask]'] = mask_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103b3b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert words to vectors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert words to vectors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m, train_data))\n\u001b[1;32m      3\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'e'"
     ]
    }
   ],
   "source": [
    "# convert words to vectors\n",
    "vectorized_text = list(map(lambda x: vocabulary[x], train_data))\n",
    "vectorized_text = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37720b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorized_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# split data into sequences of length 20\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorized_text_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvectorized_text\u001b[49m) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mlen\u001b[39m(vectorized_text) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      3\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m vectorized_text[:vectorized_text_len]\n\u001b[1;32m      4\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(vectorized_text,[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorized_text' is not defined"
     ]
    }
   ],
   "source": [
    "# split data into sequences of length 20\n",
    "vectorized_text_len = len(vectorized_text) - (len(vectorized_text) % 20)\n",
    "vectorized_text = vectorized_text[:vectorized_text_len]\n",
    "vectorized_text = np.reshape(vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34a2e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorized_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectorized_text\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorized_text' is not defined"
     ]
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0f04468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_input_label(sequence):\n",
    "    \n",
    "    # randomly choose one position in sequence to mask\n",
    "    mask = np.random.randint(low=0, high=20)\n",
    "    \n",
    "    # add mask to input\n",
    "    masked_input = [token if i != mask else mask_id for i, token in enumerate(sequence)]\n",
    "    \n",
    "    # set all values in label to -1(ignored by loss function) except the value at the masked position\n",
    "    label = [-1 if i!= mask else token for i, token in enumerate(sequence)]\n",
    "    return masked_input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "76fbd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get masked inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for seq in vectorized_text:\n",
    "    x,y = get_masked_input_label(seq)\n",
    "    inputs.append(x)\n",
    "    labels.append(y)\n",
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b6cd526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1820, 20), (1820, 20))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "623bc977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1, 3330,   -1],\n",
       "       ...,\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1, 1792,   -1]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a73b",
   "metadata": {},
   "source": [
    "### bi-directional lstm model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9830fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masked language modeling class\n",
    "class LSTM_MLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, input_length):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "        : param vocab_size : The number of unique words in the data\n",
    "        : param hidden_size   : The size of your desired RNN\n",
    "        : param embed_size : The size of your latent embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_length = input_length\n",
    "\n",
    "        ## TODO: define your trainable variables and/or layers here. This should include an\n",
    "        ## embedding component, and any other variables/layers you require.\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size+1, output_dim=self.embed_size)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "        self.dense1 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
    "\n",
    "        # fully connected linear layers\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        You must use an embedding layer as the first layer of your network (i.e. tf.nn.embedding_lookup or tf.keras.layers.Embedding)\n",
    "        :param inputs: word ids of shape (batch_size, 2)\n",
    "        :return: logits: The batch element probabilities as a tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        x = inputs\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c274c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "82/82 [==============================] - 13s 86ms/step - loss: 7.1600 - acc: 0.0049 - val_loss: 6.8625 - val_acc: 0.0052\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 5.6964 - acc: 0.0056 - val_loss: 7.1397 - val_acc: 0.0052\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 5.3775 - acc: 0.0056 - val_loss: 7.3514 - val_acc: 0.0052\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 5.1964 - acc: 0.0056 - val_loss: 8.0358 - val_acc: 0.0052\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 5.0402 - acc: 0.0056 - val_loss: 8.0269 - val_acc: 0.0052\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.8515 - acc: 0.0055 - val_loss: 8.3900 - val_acc: 0.0041\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.6261 - acc: 0.0057 - val_loss: 9.1050 - val_acc: 0.0025\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.3664 - acc: 0.0060 - val_loss: 9.4967 - val_acc: 0.0027\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 4.1173 - acc: 0.0068 - val_loss: 9.7108 - val_acc: 0.0025\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 3.8784 - acc: 0.0075 - val_loss: 10.2482 - val_acc: 0.0027\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 3.6494 - acc: 0.0083 - val_loss: 10.5441 - val_acc: 0.0025\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 3.4100 - acc: 0.0090 - val_loss: 10.8788 - val_acc: 0.0019\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 3.1634 - acc: 0.0107 - val_loss: 11.1392 - val_acc: 0.0022\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 2.9277 - acc: 0.0126 - val_loss: 11.4346 - val_acc: 0.0016\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 6s 76ms/step - loss: 2.7108 - acc: 0.0147 - val_loss: 11.8988 - val_acc: 0.0014\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 2.5048 - acc: 0.0174 - val_loss: 11.9514 - val_acc: 0.0019\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 2.2714 - acc: 0.0200 - val_loss: 12.4020 - val_acc: 0.0014\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 6s 74ms/step - loss: 2.0444 - acc: 0.0229 - val_loss: 12.6693 - val_acc: 0.0014\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 1.8405 - acc: 0.0268 - val_loss: 12.7901 - val_acc: 0.0014\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 1.6296 - acc: 0.0306 - val_loss: 12.9688 - val_acc: 0.0014\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 1.4437 - acc: 0.0340 - val_loss: 13.1367 - val_acc: 8.2418e-04\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 1.2877 - acc: 0.0354 - val_loss: 13.5050 - val_acc: 0.0011\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 1.1379 - acc: 0.0377 - val_loss: 13.7874 - val_acc: 0.0016\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.9905 - acc: 0.0402 - val_loss: 13.8969 - val_acc: 0.0014\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.9051 - acc: 0.0408 - val_loss: 14.1263 - val_acc: 0.0019\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.7965 - acc: 0.0421 - val_loss: 14.1428 - val_acc: 8.2418e-04\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.6773 - acc: 0.0439 - val_loss: 14.3249 - val_acc: 0.0011\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.5818 - acc: 0.0449 - val_loss: 14.5124 - val_acc: 0.0011\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.5101 - acc: 0.0459 - val_loss: 14.6368 - val_acc: 8.2418e-04\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.4727 - acc: 0.0459 - val_loss: 14.7187 - val_acc: 0.0014\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.4371 - acc: 0.0462 - val_loss: 14.9885 - val_acc: 0.0011\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.3822 - acc: 0.0468 - val_loss: 15.1323 - val_acc: 0.0011\n",
      "Epoch 33/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.3487 - acc: 0.0472 - val_loss: 15.2297 - val_acc: 8.2418e-04\n",
      "Epoch 34/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 0.3026 - acc: 0.0476 - val_loss: 15.3850 - val_acc: 0.0016\n",
      "Epoch 35/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 0.2595 - acc: 0.0478 - val_loss: 15.5696 - val_acc: 0.0011\n",
      "Epoch 36/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.2414 - acc: 0.0482 - val_loss: 15.5395 - val_acc: 8.2418e-04\n",
      "Epoch 37/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.2098 - acc: 0.0484 - val_loss: 15.7407 - val_acc: 2.7473e-04\n",
      "Epoch 38/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.1816 - acc: 0.0488 - val_loss: 15.8790 - val_acc: 5.4945e-04\n",
      "Epoch 39/50\n",
      "82/82 [==============================] - 6s 76ms/step - loss: 0.1775 - acc: 0.0487 - val_loss: 15.8381 - val_acc: 8.2418e-04\n",
      "Epoch 40/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1646 - acc: 0.0489 - val_loss: 16.0646 - val_acc: 5.4945e-04\n",
      "Epoch 41/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1724 - acc: 0.0487 - val_loss: 16.1265 - val_acc: 8.2418e-04\n",
      "Epoch 42/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.1622 - acc: 0.0490 - val_loss: 16.0373 - val_acc: 5.4945e-04\n",
      "Epoch 43/50\n",
      "82/82 [==============================] - 6s 74ms/step - loss: 0.1368 - acc: 0.0491 - val_loss: 16.1991 - val_acc: 5.4945e-04\n",
      "Epoch 44/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1270 - acc: 0.0491 - val_loss: 16.3785 - val_acc: 2.7473e-04\n",
      "Epoch 45/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.1120 - acc: 0.0495 - val_loss: 16.3501 - val_acc: 2.7473e-04\n",
      "Epoch 46/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.1219 - acc: 0.0492 - val_loss: 16.5607 - val_acc: 2.7473e-04\n",
      "Epoch 47/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.0967 - acc: 0.0494 - val_loss: 16.5257 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0742 - acc: 0.0498 - val_loss: 16.6474 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0606 - acc: 0.0498 - val_loss: 16.7899 - val_acc: 2.7473e-04\n",
      "Epoch 50/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0582 - acc: 0.0498 - val_loss: 16.8156 - val_acc: 5.4945e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd82f3788b0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_MLM(vocab_size, 64, 20)\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "model.compile(loss=loss_metric, optimizer='adam', metrics=['acc'])\n",
    "model.fit(x=inputs, y=labels, validation_split=0.1, batch_size=20, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "79ca807a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'embedding_13' (type Embedding).\n\n'list' object has no attribute 'dtype'\n\nCall arguments received by layer 'embedding_13' (type Embedding):\n  • inputs=['3577', '3205']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make prediction: still in progress\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#query = ['emperor', 'like', '[mask]', 'cloth', 'dress']\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#query_id = [vocabulary[q] for q in query]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query_id \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pred\n",
      "File \u001b[0;32m/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[116], line 38\u001b[0m, in \u001b[0;36mLSTM_MLM.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# embedding layer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'embedding_13' (type Embedding).\n\n'list' object has no attribute 'dtype'\n\nCall arguments received by layer 'embedding_13' (type Embedding):\n  • inputs=['3577', '3205']"
     ]
    }
   ],
   "source": [
    "# make prediction: still in progress\n",
    "\n",
    "#query = ['emperor', 'like', '[mask]', 'cloth', 'dress']\n",
    "#query_id = [vocabulary[q] for q in query]\n",
    "query_id = x[0:2]\n",
    "\n",
    "pred = model(query_id)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6969cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d696ff06",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aa8ac",
   "metadata": {},
   "source": [
    "references: \"Attention Is All You Need\" paper by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head self-attention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embed_dim % self.num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f12b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, input_length, embed_dim, num_heads, ff_dim, num_blocks, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)\n",
    "        self.pos_encoding = self.positional_encoding(input_length, embed_dim)\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_blocks)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def positional_encoding(self, input_length, embed_dim):\n",
    "        angles = 1 / (10000 ** (tf.range(0, embed_dim, 2, dtype=tf.float32) / embed_dim))\n",
    "        pos_encodings = tf.range(input_length, dtype=tf.float32)[:, tf.newaxis] * angles\n",
    "        pos_encodings = tf.concat([tf.sin(pos_encodings), tf.cos(pos_encodings)], axis=-1)\n",
    "        pos_encodings = pos_encodings[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encodings, tf.float32)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        seq_length = inputs.shape[1]\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32))\n",
    "        embeddings += self.pos_encoding[:, :seq_length, :]\n",
    "\n",
    "        x = self.dropout(embeddings, training=training)\n",
    "\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training)\n",
    "\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64138dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "input_length = 20\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_blocks = 4\n",
    "\n",
    "# Instantiate the model\n",
    "model_t = Transformer(vocab_size, input_length, embed_dim, num_heads, ff_dim, num_blocks)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, ignore_index=-1)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model_t.compile(loss=loss_metric, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_t.fit(x=inputs, y=labels, validation_split=0.1, batch_size=20, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
