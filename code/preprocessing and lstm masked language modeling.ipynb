{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e45b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad03035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 13:47:33.014320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/selinawang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/selinawang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c30d69",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "- removed gender pronouns from list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c181",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c08905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"THE EMPEROR'S NEW CLOTHES\\n\", '\\n', 'Many years ago, there was an Emperor, who was so excessively fond of\\n', 'new clothes, that he spent all his money in dress. He did not trouble\\n', 'himself in the least about his soldiers; nor did he care to go either to\\n', 'the theatre or the chase, except for the opportunities then afforded him\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/pg1597.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa27d9",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3e8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    data = \"\".join(line)\n",
    "    data = word_tokenize(data)\n",
    "    words = \" \".join(data)\n",
    "    lower_w = words.lower()\n",
    "    \n",
    "    # keep gender pronouns in stop words\n",
    "    stop_words_keep = ['he',\n",
    "     'him',\n",
    "     'his',\n",
    "     'himself',\n",
    "     'she',\n",
    "     \"she's\",\n",
    "     'her',\n",
    "     'hers',\n",
    "     'herself']\n",
    "    stop_words = [word for word in stopwords.words('english') if word not in stop_words_keep]\n",
    "\n",
    "    stop_words = set(stop_words) \n",
    "    word_tokens = lower_w.split(\" \")\n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    output = []\n",
    "    for word in filtered_sentence:\n",
    "        output.append((ps.stem(word)))\n",
    "    final_out = \" \".join(output)\n",
    "    final_out = final_out.replace('\\n', ' ').split(' ')\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c73d4",
   "metadata": {},
   "source": [
    "### get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "39342b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    words = list(set(data))\n",
    "    vocabulary = {word:index for index, word in enumerate(words)}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    return vocabulary, vocab_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0ed439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(lines)\n",
    "vocabulary, vocab_size = get_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "021aeda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4126,\n",
       " {'among': 0,\n",
       "  'bank': 1,\n",
       "  'bout': 2,\n",
       "  'fata': 3,\n",
       "  'ascend': 4,\n",
       "  'wear': 5,\n",
       "  'bearer': 6,\n",
       "  'crumbl': 7,\n",
       "  'rose': 8,\n",
       "  'crack': 9,\n",
       "  'water-pl': 10,\n",
       "  'grain': 11,\n",
       "  'weather-cock': 12,\n",
       "  'climb': 13,\n",
       "  'soundli': 14,\n",
       "  'purchas': 15,\n",
       "  'hurl': 16,\n",
       "  'absent': 17,\n",
       "  'a.': 18,\n",
       "  'leapt': 19,\n",
       "  'cup': 20,\n",
       "  'couldst': 21,\n",
       "  'militia': 22,\n",
       "  'uppermost': 23,\n",
       "  'tread': 24,\n",
       "  'half-dress': 25,\n",
       "  'colder': 26,\n",
       "  'went': 27,\n",
       "  'belov': 28,\n",
       "  'exist': 29,\n",
       "  'frozen': 30,\n",
       "  'throne': 31,\n",
       "  'rabbit': 32,\n",
       "  'shelv': 33,\n",
       "  'lid': 34,\n",
       "  'becom': 35,\n",
       "  'freez': 36,\n",
       "  'farthest': 37,\n",
       "  '1484': 38,\n",
       "  'wild-dron': 39,\n",
       "  'mighti': 40,\n",
       "  'china': 41,\n",
       "  'spinning-wheel': 42,\n",
       "  'crafti': 43,\n",
       "  'encount': 44,\n",
       "  'stair': 45,\n",
       "  'queen': 46,\n",
       "  'hastili': 47,\n",
       "  'signifi': 48,\n",
       "  'asund': 49,\n",
       "  'vein': 50,\n",
       "  'nurs': 51,\n",
       "  'adjoin': 52,\n",
       "  'cold': 53,\n",
       "  'imbib': 54,\n",
       "  'supple.': 55,\n",
       "  'sake': 56,\n",
       "  'rate': 57,\n",
       "  'tell': 58,\n",
       "  'scrubbi': 59,\n",
       "  'unknown': 60,\n",
       "  'terrif': 61,\n",
       "  'brim': 62,\n",
       "  'feel': 63,\n",
       "  'heel': 64,\n",
       "  'ruin': 65,\n",
       "  'soe': 66,\n",
       "  'garter': 67,\n",
       "  'anxiou': 68,\n",
       "  'templ': 69,\n",
       "  'endur': 70,\n",
       "  'christmas-tre': 71,\n",
       "  'favor': 72,\n",
       "  'stateli': 73,\n",
       "  'safe': 74,\n",
       "  'warmest': 75,\n",
       "  'furthest': 76,\n",
       "  'humbl': 77,\n",
       "  'smaller': 78,\n",
       "  'ddsa': 79,\n",
       "  'women': 80,\n",
       "  'metropoli': 81,\n",
       "  'hedg': 82,\n",
       "  'grove': 83,\n",
       "  'romant': 84,\n",
       "  'solemn': 85,\n",
       "  'motion': 86,\n",
       "  'publish': 87,\n",
       "  'triangl': 88,\n",
       "  'fountain': 89,\n",
       "  'punch': 90,\n",
       "  'listening.': 91,\n",
       "  'accustom': 92,\n",
       "  'way': 93,\n",
       "  'manag': 94,\n",
       "  'councillor': 95,\n",
       "  'king': 96,\n",
       "  'nearer': 97,\n",
       "  'happi': 98,\n",
       "  'spoke': 99,\n",
       "  'await': 100,\n",
       "  'attun': 101,\n",
       "  'snow-wreath': 102,\n",
       "  'myrtle-branch': 103,\n",
       "  'occup': 104,\n",
       "  'music-director': 105,\n",
       "  'apothegm': 106,\n",
       "  'tea-parti': 107,\n",
       "  'directli': 108,\n",
       "  'straight': 109,\n",
       "  'sugarplum': 110,\n",
       "  'island': 111,\n",
       "  'pronoun': 112,\n",
       "  'highest': 113,\n",
       "  'rhyme': 114,\n",
       "  'counten': 115,\n",
       "  'complain': 116,\n",
       "  'every-day': 117,\n",
       "  'clay-pip': 118,\n",
       "  'charact': 119,\n",
       "  'deepli': 120,\n",
       "  'wast': 121,\n",
       "  'salmon': 122,\n",
       "  'bread': 123,\n",
       "  'empti': 124,\n",
       "  'dim': 125,\n",
       "  'soliloqu': 126,\n",
       "  'bedroom': 127,\n",
       "  'magnifi': 128,\n",
       "  'quiver': 129,\n",
       "  'flat': 130,\n",
       "  \"hog's-leath\": 131,\n",
       "  ';': 132,\n",
       "  'instrument': 133,\n",
       "  'nineteen': 134,\n",
       "  'plump': 135,\n",
       "  'angri': 136,\n",
       "  'desert': 137,\n",
       "  'ever-lov': 138,\n",
       "  'insignific': 139,\n",
       "  'brass-wir': 140,\n",
       "  'mount': 141,\n",
       "  'shrug': 142,\n",
       "  'music': 143,\n",
       "  \"i'v\": 144,\n",
       "  'bustl': 145,\n",
       "  'flutter': 146,\n",
       "  'passport': 147,\n",
       "  ':': 148,\n",
       "  'tingl': 149,\n",
       "  'dread': 150,\n",
       "  'spare': 151,\n",
       "  'inscript': 152,\n",
       "  'beard': 153,\n",
       "  'cranki': 154,\n",
       "  'get': 155,\n",
       "  'disgrac': 156,\n",
       "  'swine': 157,\n",
       "  'merit': 158,\n",
       "  'unbecom': 159,\n",
       "  'brighter': 160,\n",
       "  'glove': 161,\n",
       "  'tenement': 162,\n",
       "  'press': 163,\n",
       "  'bee': 164,\n",
       "  'robe': 165,\n",
       "  'swung': 166,\n",
       "  'unless': 167,\n",
       "  'gradual': 168,\n",
       "  'might': 169,\n",
       "  'decor': 170,\n",
       "  'night': 171,\n",
       "  'butter': 172,\n",
       "  'could': 173,\n",
       "  'sneez': 174,\n",
       "  'de': 175,\n",
       "  'confidenti': 176,\n",
       "  'burst': 177,\n",
       "  'charmant': 178,\n",
       "  'farewel': 179,\n",
       "  'claret': 180,\n",
       "  'groan': 181,\n",
       "  'darkest': 182,\n",
       "  'nut': 183,\n",
       "  'driven': 184,\n",
       "  'wink': 185,\n",
       "  'these.': 186,\n",
       "  'galosh': 187,\n",
       "  'necessari': 188,\n",
       "  'survey': 189,\n",
       "  'owner': 190,\n",
       "  'herd': 191,\n",
       "  'carv': 192,\n",
       "  'clumsi': 193,\n",
       "  'weg': 194,\n",
       "  'appleblossom': 195,\n",
       "  'intim': 196,\n",
       "  'tablecloth': 197,\n",
       "  'perish': 198,\n",
       "  'nybod': 199,\n",
       "  'driest': 200,\n",
       "  'beech-wood': 201,\n",
       "  'clean': 202,\n",
       "  'thrown': 203,\n",
       "  'suit': 204,\n",
       "  'gloom': 205,\n",
       "  'masquerad': 206,\n",
       "  'wors': 207,\n",
       "  'parent': 208,\n",
       "  'prefer': 209,\n",
       "  'distinctli': 210,\n",
       "  'flown': 211,\n",
       "  'leg': 212,\n",
       "  'disagre': 213,\n",
       "  'sunshin': 214,\n",
       "  'vain': 215,\n",
       "  'curiou': 216,\n",
       "  'touchingli': 217,\n",
       "  'forest': 218,\n",
       "  'preserv': 219,\n",
       "  'interrupt': 220,\n",
       "  'easy-chair': 221,\n",
       "  'scholar': 222,\n",
       "  'canari': 223,\n",
       "  'oneself': 224,\n",
       "  'contin': 225,\n",
       "  'silent': 226,\n",
       "  'poesi': 227,\n",
       "  'noon-day': 228,\n",
       "  'relat': 229,\n",
       "  'inquisit': 230,\n",
       "  'form': 231,\n",
       "  'caldrons.': 232,\n",
       "  'fantast': 233,\n",
       "  'food': 234,\n",
       "  'larger': 235,\n",
       "  \"author'\": 236,\n",
       "  'translat': 237,\n",
       "  'must': 238,\n",
       "  'thi': 239,\n",
       "  'hardli': 240,\n",
       "  'window-shutt': 241,\n",
       "  \"saw'st\": 242,\n",
       "  'foggi': 243,\n",
       "  'debt': 244,\n",
       "  'bear': 245,\n",
       "  'lath': 246,\n",
       "  'crept': 247,\n",
       "  'paper': 248,\n",
       "  'manor-hous': 249,\n",
       "  'everyth': 250,\n",
       "  'also': 251,\n",
       "  'hurri': 252,\n",
       "  'seaman': 253,\n",
       "  'comfortless': 254,\n",
       "  'block': 255,\n",
       "  'grass': 256,\n",
       "  'pull': 257,\n",
       "  'encompass': 258,\n",
       "  'futur': 259,\n",
       "  'limit': 260,\n",
       "  'can.': 261,\n",
       "  'sky': 262,\n",
       "  'draught': 263,\n",
       "  'bite': 264,\n",
       "  'danger': 265,\n",
       "  'ordain': 266,\n",
       "  'quay': 267,\n",
       "  'dock-leav': 268,\n",
       "  'come': 269,\n",
       "  'prettier': 270,\n",
       "  'mane': 271,\n",
       "  'trip': 272,\n",
       "  'content': 273,\n",
       "  'caldron': 274,\n",
       "  'shrivel': 275,\n",
       "  'everybodi': 276,\n",
       "  'stalk': 277,\n",
       "  'will': 278,\n",
       "  'lantern': 279,\n",
       "  'dwelt': 280,\n",
       "  'top': 281,\n",
       "  'sober': 282,\n",
       "  'court': 283,\n",
       "  'mummeri': 284,\n",
       "  'weather': 285,\n",
       "  'speech': 286,\n",
       "  'threateningli': 287,\n",
       "  'past': 288,\n",
       "  'griev': 289,\n",
       "  'ape': 290,\n",
       "  'wildli': 291,\n",
       "  'preliminari': 292,\n",
       "  'idea': 293,\n",
       "  'load': 294,\n",
       "  'lake': 295,\n",
       "  'privi': 296,\n",
       "  'kiss': 297,\n",
       "  'summer': 298,\n",
       "  'equipag': 299,\n",
       "  'anew': 300,\n",
       "  'loud': 301,\n",
       "  'creatur': 302,\n",
       "  'us.': 303,\n",
       "  'acknowledg': 304,\n",
       "  'rose-tre': 305,\n",
       "  'world': 306,\n",
       "  'nanny-goat': 307,\n",
       "  'happier': 308,\n",
       "  'shopman': 309,\n",
       "  'initi': 310,\n",
       "  'tomorrow': 311,\n",
       "  'jerk': 312,\n",
       "  'clung': 313,\n",
       "  'fold': 314,\n",
       "  'courteous': 315,\n",
       "  'starve.': 316,\n",
       "  'peter': 317,\n",
       "  'neat': 318,\n",
       "  'huge': 319,\n",
       "  'little.': 320,\n",
       "  'bride': 321,\n",
       "  'wake': 322,\n",
       "  'individu': 323,\n",
       "  'caw': 324,\n",
       "  'below.': 325,\n",
       "  'generos': 326,\n",
       "  'chest': 327,\n",
       "  'acquaint': 328,\n",
       "  'passer-bi': 329,\n",
       "  'benefit': 330,\n",
       "  'fare': 331,\n",
       "  \"aunt'\": 332,\n",
       "  'excellenza': 333,\n",
       "  'coin': 334,\n",
       "  'distort': 335,\n",
       "  'porcelain': 336,\n",
       "  'pick': 337,\n",
       "  'explain': 338,\n",
       "  'rough': 339,\n",
       "  'ranunculu': 340,\n",
       "  'soothsay': 341,\n",
       "  'shear': 342,\n",
       "  'narrow': 343,\n",
       "  'militari': 344,\n",
       "  'sung': 345,\n",
       "  'heiberg': 346,\n",
       "  'famili': 347,\n",
       "  'multipl': 348,\n",
       "  'dissatisfact': 349,\n",
       "  'roll': 350,\n",
       "  'pageant': 351,\n",
       "  'better': 352,\n",
       "  'elderflow': 353,\n",
       "  'inform': 354,\n",
       "  'abus': 355,\n",
       "  'rheumat': 356,\n",
       "  'separ': 357,\n",
       "  'bid': 358,\n",
       "  'rounder': 359,\n",
       "  'rais': 360,\n",
       "  'jeer': 361,\n",
       "  'twist': 362,\n",
       "  'yellow': 363,\n",
       "  'comrad': 364,\n",
       "  'bodili': 365,\n",
       "  'clear-sight': 366,\n",
       "  'knap': 367,\n",
       "  'gigant': 368,\n",
       "  'garret': 369,\n",
       "  'snuf': 370,\n",
       "  'els': 371,\n",
       "  'upon': 372,\n",
       "  'shore': 373,\n",
       "  'game': 374,\n",
       "  'field': 375,\n",
       "  'constantli': 376,\n",
       "  'aurora': 377,\n",
       "  'boaster': 378,\n",
       "  'tabl': 379,\n",
       "  'connect': 380,\n",
       "  \"'thou\": 381,\n",
       "  'ringstead': 382,\n",
       "  'gutter': 383,\n",
       "  'certainli': 384,\n",
       "  'select': 385,\n",
       "  'freed': 386,\n",
       "  'eratostratu': 387,\n",
       "  'influenc': 388,\n",
       "  'pillori': 389,\n",
       "  'temper': 390,\n",
       "  'haycock': 391,\n",
       "  'beyond': 392,\n",
       "  'bed': 393,\n",
       "  'squeak': 394,\n",
       "  'girdl': 395,\n",
       "  ',': 396,\n",
       "  'dazzl': 397,\n",
       "  'mild': 398,\n",
       "  'peasant': 399,\n",
       "  'pussy-cat': 400,\n",
       "  'mound': 401,\n",
       "  'days.': 402,\n",
       "  'knelt': 403,\n",
       "  'anniversari': 404,\n",
       "  'prayer-book': 405,\n",
       "  'cost': 406,\n",
       "  'buoyant': 407,\n",
       "  'remot': 408,\n",
       "  'fredericksburg': 409,\n",
       "  'saloon': 410,\n",
       "  'realiz': 411,\n",
       "  'oracular': 412,\n",
       "  'absalon': 413,\n",
       "  'gain': 414,\n",
       "  'room.': 415,\n",
       "  'funer': 416,\n",
       "  'thrust': 417,\n",
       "  'trateratra': 418,\n",
       "  'hind': 419,\n",
       "  'present': 420,\n",
       "  'built': 421,\n",
       "  'counter': 422,\n",
       "  'peep-hol': 423,\n",
       "  'purs': 424,\n",
       "  'burn': 425,\n",
       "  'vetturino': 426,\n",
       "  'hroar': 427,\n",
       "  'sunbeam': 428,\n",
       "  'town-gat': 429,\n",
       "  'here.': 430,\n",
       "  'feather': 431,\n",
       "  'novel': 432,\n",
       "  'stir': 433,\n",
       "  'fortune-tel': 434,\n",
       "  'iii': 435,\n",
       "  'confus': 436,\n",
       "  'man.': 437,\n",
       "  'shudder': 438,\n",
       "  'femal': 439,\n",
       "  'topic': 440,\n",
       "  'cairn': 441,\n",
       "  'sea': 442,\n",
       "  'ill-humor': 443,\n",
       "  'draw': 444,\n",
       "  'imperi': 445,\n",
       "  'proudli': 446,\n",
       "  'move': 447,\n",
       "  'present.': 448,\n",
       "  'cluster': 449,\n",
       "  'himself': 450,\n",
       "  'anecdot': 451,\n",
       "  'suspicion': 452,\n",
       "  'ala': 453,\n",
       "  'employ': 454,\n",
       "  'hidden': 455,\n",
       "  'cleanli': 456,\n",
       "  'lili': 457,\n",
       "  'refreshing.': 458,\n",
       "  'hoars': 459,\n",
       "  'eye': 460,\n",
       "  'throughout': 461,\n",
       "  'impostor': 462,\n",
       "  'practis': 463,\n",
       "  'abandon': 464,\n",
       "  'young': 465,\n",
       "  'loom': 466,\n",
       "  'heart': 467,\n",
       "  '?': 468,\n",
       "  'flesh': 469,\n",
       "  'scanti': 470,\n",
       "  'ceremoni': 471,\n",
       "  'pattern': 472,\n",
       "  'grand-children': 473,\n",
       "  'give': 474,\n",
       "  'hoar-frost': 475,\n",
       "  'inner': 476,\n",
       "  'rang': 477,\n",
       "  'large-s': 478,\n",
       "  'clearli': 479,\n",
       "  'furthermor': 480,\n",
       "  'augusta': 481,\n",
       "  'inspect': 482,\n",
       "  'bosom': 483,\n",
       "  'cook': 484,\n",
       "  'them.': 485,\n",
       "  'leader': 486,\n",
       "  'rope': 487,\n",
       "  'daughter.': 488,\n",
       "  'occupi': 489,\n",
       "  'moon-stricken': 490,\n",
       "  'judgment': 491,\n",
       "  'ingemann': 492,\n",
       "  'wive': 493,\n",
       "  \"holberg'\": 494,\n",
       "  'support': 495,\n",
       "  'gehmen.': 496,\n",
       "  'starlit': 497,\n",
       "  'canal': 498,\n",
       "  'pitch-black': 499,\n",
       "  'destin': 500,\n",
       "  'train': 501,\n",
       "  'bubbl': 502,\n",
       "  'execution.': 503,\n",
       "  'suppli': 504,\n",
       "  'circl': 505,\n",
       "  'sixth': 506,\n",
       "  \"'ll\": 507,\n",
       "  'shock': 508,\n",
       "  'rock.': 509,\n",
       "  'common': 510,\n",
       "  'water-tub': 511,\n",
       "  'wardrobe.': 512,\n",
       "  'lose': 513,\n",
       "  'spitzbergen.': 514,\n",
       "  'occur': 515,\n",
       "  'whatev': 516,\n",
       "  'howev': 517,\n",
       "  'pew': 518,\n",
       "  'fifty-two': 519,\n",
       "  'thirsti': 520,\n",
       "  'proper': 521,\n",
       "  'balustrad': 522,\n",
       "  'statu': 523,\n",
       "  'schnipp-schnapp-schnurre-basselurr': 524,\n",
       "  'neighborhood': 525,\n",
       "  'push': 526,\n",
       "  'henc': 527,\n",
       "  'bred': 528,\n",
       "  'adventur': 529,\n",
       "  'moveth': 530,\n",
       "  'familiar': 531,\n",
       "  'storm': 532,\n",
       "  'head': 533,\n",
       "  'motley': 534,\n",
       "  'taught': 535,\n",
       "  'arrog': 536,\n",
       "  'breathlessli': 537,\n",
       "  'ask': 538,\n",
       "  'respir': 539,\n",
       "  'copenhagen': 540,\n",
       "  'crumpl': 541,\n",
       "  'son': 542,\n",
       "  'hop': 543,\n",
       "  'misfortun': 544,\n",
       "  'great': 545,\n",
       "  'measur': 546,\n",
       "  'sink': 547,\n",
       "  'thyself': 548,\n",
       "  'witti': 549,\n",
       "  'stout': 550,\n",
       "  'feudal': 551,\n",
       "  'comparison': 552,\n",
       "  'spruce': 553,\n",
       "  'hammer': 554,\n",
       "  'bouquet': 555,\n",
       "  'door-keep': 556,\n",
       "  'mint': 557,\n",
       "  'ground': 558,\n",
       "  'rejoin': 559,\n",
       "  'beautiful.': 560,\n",
       "  'bird': 561,\n",
       "  'morn': 562,\n",
       "  'tomcat': 563,\n",
       "  'cheek': 564,\n",
       "  'public-hous': 565,\n",
       "  'extend': 566,\n",
       "  'tint': 567,\n",
       "  'whistl': 568,\n",
       "  'pale': 569,\n",
       "  'therein': 570,\n",
       "  'church-bel': 571,\n",
       "  'cathedr': 572,\n",
       "  'sorb': 573,\n",
       "  'iron': 574,\n",
       "  \"sang'st\": 575,\n",
       "  'waterpail': 576,\n",
       "  'destini': 577,\n",
       "  'creation': 578,\n",
       "  'crackl': 579,\n",
       "  'rust': 580,\n",
       "  'wind': 581,\n",
       "  'heroin': 582,\n",
       "  \"gentlemen'\": 583,\n",
       "  'mutter': 584,\n",
       "  'vapor-bath': 585,\n",
       "  'africa': 586,\n",
       "  'nowher': 587,\n",
       "  'fulfil': 588,\n",
       "  'pavement': 589,\n",
       "  'ungodli': 590,\n",
       "  'tendril': 591,\n",
       "  'trifl': 592,\n",
       "  'someon': 593,\n",
       "  'enough': 594,\n",
       "  'mere': 595,\n",
       "  'cupboard': 596,\n",
       "  'round': 597,\n",
       "  'relationship': 598,\n",
       "  'said': 599,\n",
       "  'hot': 600,\n",
       "  'cloud': 601,\n",
       "  'northern': 602,\n",
       "  'overshadow': 603,\n",
       "  'prophet': 604,\n",
       "  'man': 605,\n",
       "  'meant': 606,\n",
       "  'secur': 607,\n",
       "  'elfin': 608,\n",
       "  'care': 609,\n",
       "  'burnt-out': 610,\n",
       "  'geographi': 611,\n",
       "  'tailor': 612,\n",
       "  'swollen': 613,\n",
       "  'wrist': 614,\n",
       "  'realiti': 615,\n",
       "  'tea-pot': 616,\n",
       "  'chees': 617,\n",
       "  'cosmopolit': 618,\n",
       "  'rapid': 619,\n",
       "  'obtain': 620,\n",
       "  'mani': 621,\n",
       "  'etna': 622,\n",
       "  'unhappy.': 623,\n",
       "  'suppl': 624,\n",
       "  'twelv': 625,\n",
       "  'good-by': 626,\n",
       "  'saunter': 627,\n",
       "  'her': 628,\n",
       "  'maiden': 629,\n",
       "  'autumn': 630,\n",
       "  'coach': 631,\n",
       "  'lapland': 632,\n",
       "  'bad': 633,\n",
       "  'flush': 634,\n",
       "  'bump': 635,\n",
       "  'tini': 636,\n",
       "  'recollect': 637,\n",
       "  'rosi': 638,\n",
       "  'court.': 639,\n",
       "  'bella': 640,\n",
       "  'bewar': 641,\n",
       "  'accomplish': 642,\n",
       "  'madonna': 643,\n",
       "  'raven': 644,\n",
       "  'condit': 645,\n",
       "  'rid': 646,\n",
       "  'score': 647,\n",
       "  'life': 648,\n",
       "  'seldom': 649,\n",
       "  'ash': 650,\n",
       "  'author': 651,\n",
       "  'untru': 652,\n",
       "  'magic': 653,\n",
       "  'majesti': 654,\n",
       "  'discharg': 655,\n",
       "  'tar': 656,\n",
       "  'theologian': 657,\n",
       "  'triumphant': 658,\n",
       "  'thinner': 659,\n",
       "  '2000': 660,\n",
       "  'trod': 661,\n",
       "  'throat': 662,\n",
       "  'cupola': 663,\n",
       "  'holiday': 664,\n",
       "  'rise': 665,\n",
       "  'path': 666,\n",
       "  'seat': 667,\n",
       "  'extent': 668,\n",
       "  'miser': 669,\n",
       "  'skull': 670,\n",
       "  'flaminiu': 671,\n",
       "  'pith': 672,\n",
       "  'speci': 673,\n",
       "  'commiss': 674,\n",
       "  'squar': 675,\n",
       "  'richli': 676,\n",
       "  'mountain-ridg': 677,\n",
       "  'pillow': 678,\n",
       "  'wrong': 679,\n",
       "  'simplest': 680,\n",
       "  'bless': 681,\n",
       "  'ach': 682,\n",
       "  'shut': 683,\n",
       "  'stuff.': 684,\n",
       "  'amongst': 685,\n",
       "  'trust': 686,\n",
       "  'handwrit': 687,\n",
       "  'modest': 688,\n",
       "  'interchang': 689,\n",
       "  'ugh': 690,\n",
       "  'eve': 691,\n",
       "  'cupid': 692,\n",
       "  'ten': 693,\n",
       "  'discret': 694,\n",
       "  'hither': 695,\n",
       "  'millin': 696,\n",
       "  'a.d.': 697,\n",
       "  'caught': 698,\n",
       "  'birch': 699,\n",
       "  'habit': 700,\n",
       "  'holberg': 701,\n",
       "  'word.': 702,\n",
       "  'sum': 703,\n",
       "  'carriag': 704,\n",
       "  'overcom': 705,\n",
       "  \"cocks'-comb\": 706,\n",
       "  'cask': 707,\n",
       "  'glare': 708,\n",
       "  'casket': 709,\n",
       "  'pretti': 710,\n",
       "  'wiser': 711,\n",
       "  'beckon': 712,\n",
       "  'match': 713,\n",
       "  'smile': 714,\n",
       "  'bedchamb': 715,\n",
       "  'mis-shapen': 716,\n",
       "  'wander': 717,\n",
       "  'berri': 718,\n",
       "  'saw': 719,\n",
       "  'hous': 720,\n",
       "  'anger': 721,\n",
       "  'knee': 722,\n",
       "  'mast': 723,\n",
       "  'great-grand-children': 724,\n",
       "  'formerli': 725,\n",
       "  'sad': 726,\n",
       "  'nail': 727,\n",
       "  'absorb': 728,\n",
       "  'clock': 729,\n",
       "  'tast': 730,\n",
       "  \"'it\": 731,\n",
       "  'partner': 732,\n",
       "  'geniu': 733,\n",
       "  'cherri': 734,\n",
       "  'spinach': 735,\n",
       "  'age': 736,\n",
       "  'diamond': 737,\n",
       "  'fork': 738,\n",
       "  'inde': 739,\n",
       "  'poodle-dog': 740,\n",
       "  'doubtless': 741,\n",
       "  'result': 742,\n",
       "  'bread-fruit': 743,\n",
       "  'sir': 744,\n",
       "  'ice': 745,\n",
       "  'unnecessari': 746,\n",
       "  'voic': 747,\n",
       "  'chair': 748,\n",
       "  'recal': 749,\n",
       "  'sit': 750,\n",
       "  'pair': 751,\n",
       "  'whisper': 752,\n",
       "  'pilgrimag': 753,\n",
       "  'eternally-creak': 754,\n",
       "  'gurgl': 755,\n",
       "  'evergreen': 756,\n",
       "  'kickery-ki': 757,\n",
       "  'kindli': 758,\n",
       "  'chase': 759,\n",
       "  'forth': 760,\n",
       "  'becam': 761,\n",
       "  'immediately.': 762,\n",
       "  'impress': 763,\n",
       "  'half': 764,\n",
       "  'lighten': 765,\n",
       "  'spark': 766,\n",
       "  'proprietor': 767,\n",
       "  'prude': 768,\n",
       "  'larder': 769,\n",
       "  'shower': 770,\n",
       "  'bye': 771,\n",
       "  \"'twould\": 772,\n",
       "  'paint': 773,\n",
       "  'stock': 774,\n",
       "  'clay-floor': 775,\n",
       "  'breath': 776,\n",
       "  'scarc': 777,\n",
       "  'boreali': 778,\n",
       "  'seventh': 779,\n",
       "  'vexat': 780,\n",
       "  'uncomb': 781,\n",
       "  'thu': 782,\n",
       "  \"'to\": 783,\n",
       "  'electricity.': 784,\n",
       "  'titl': 785,\n",
       "  'thundercloud': 786,\n",
       "  'sailor-boy': 787,\n",
       "  'thyme': 788,\n",
       "  'whiter': 789,\n",
       "  'better.': 790,\n",
       "  'address': 791,\n",
       "  'christianshafen.': 792,\n",
       "  'halberd': 793,\n",
       "  'cri': 794,\n",
       "  'avail': 795,\n",
       "  'deck': 796,\n",
       "  'famous': 797,\n",
       "  'consolingli': 798,\n",
       "  'rush': 799,\n",
       "  'eleg': 800,\n",
       "  'kribledi': 801,\n",
       "  'widow-ladi': 802,\n",
       "  'lieuten': 803,\n",
       "  'promenad': 804,\n",
       "  'unti': 805,\n",
       "  'lead': 806,\n",
       "  'wipe': 807,\n",
       "  'remain.': 808,\n",
       "  'serious': 809,\n",
       "  'impati': 810,\n",
       "  'long-head': 811,\n",
       "  'torch': 812,\n",
       "  'landlord': 813,\n",
       "  'nay': 814,\n",
       "  'trap': 815,\n",
       "  'glassi': 816,\n",
       "  'walking-sho': 817,\n",
       "  'loudli': 818,\n",
       "  'state': 819,\n",
       "  'bac': 820,\n",
       "  'return': 821,\n",
       "  'delici': 822,\n",
       "  'clerk': 823,\n",
       "  'anxieti': 824,\n",
       "  'herbag': 825,\n",
       "  'balcony-door': 826,\n",
       "  'unperceiv': 827,\n",
       "  'allow': 828,\n",
       "  'anywher': 829,\n",
       "  'equal': 830,\n",
       "  'hottest': 831,\n",
       "  'cower': 832,\n",
       "  'leav': 833,\n",
       "  'bawl': 834,\n",
       "  'lest': 835,\n",
       "  'meal': 836,\n",
       "  'consort': 837,\n",
       "  'slyli': 838,\n",
       "  'teapot': 839,\n",
       "  'away': 840,\n",
       "  \"say'st\": 841,\n",
       "  'born': 842,\n",
       "  'glimmer': 843,\n",
       "  'tower': 844,\n",
       "  'freshen': 845,\n",
       "  'pale-r': 846,\n",
       "  'wild-fowl': 847,\n",
       "  'wise': 848,\n",
       "  'denmark': 849,\n",
       "  'truli': 850,\n",
       "  'herself': 851,\n",
       "  'plumtre': 852,\n",
       "  'inch': 853,\n",
       "  'salt': 854,\n",
       "  'tore': 855,\n",
       "  'retain': 856,\n",
       "  'sing': 857,\n",
       "  'rode': 858,\n",
       "  \"'ti\": 859,\n",
       "  'complet': 860,\n",
       "  \"n't\": 861,\n",
       "  'depict': 862,\n",
       "  'tremend': 863,\n",
       "  'daughter': 864,\n",
       "  'watchman': 865,\n",
       "  'well-known': 866,\n",
       "  'hare': 867,\n",
       "  'still': 868,\n",
       "  'broader-should': 869,\n",
       "  'arriv': 870,\n",
       "  'filthiest': 871,\n",
       "  'bay': 872,\n",
       "  'hunger': 873,\n",
       "  'courtyard': 874,\n",
       "  'â€œ': 875,\n",
       "  'appetit': 876,\n",
       "  'grasshopp': 877,\n",
       "  'church': 878,\n",
       "  'lowest': 879,\n",
       "  'squeez': 880,\n",
       "  'cobbler': 881,\n",
       "  'nobl': 882,\n",
       "  'matters.': 883,\n",
       "  'upright': 884,\n",
       "  'damp': 885,\n",
       "  'sentri': 886,\n",
       "  'v.': 887,\n",
       "  'uncommonli': 888,\n",
       "  'betroth': 889,\n",
       "  'anoth': 890,\n",
       "  'bottl': 891,\n",
       "  'egress': 892,\n",
       "  'wilt': 893,\n",
       "  'unmanag': 894,\n",
       "  'sunni': 895,\n",
       "  'effectu': 896,\n",
       "  'hue': 897,\n",
       "  'waistcoat': 898,\n",
       "  'illumin': 899,\n",
       "  'tone': 900,\n",
       "  'leap-frog': 901,\n",
       "  'middle-ag': 902,\n",
       "  'hearest': 903,\n",
       "  'toward': 904,\n",
       "  'oppress': 905,\n",
       "  'deem': 906,\n",
       "  'distanc': 907,\n",
       "  '!': 908,\n",
       "  'someth': 909,\n",
       "  \"'dryad\": 910,\n",
       "  'singular': 911,\n",
       "  'all': 912,\n",
       "  'repres': 913,\n",
       "  'sight': 914,\n",
       "  'long-intend': 915,\n",
       "  'raini': 916,\n",
       "  'till': 917,\n",
       "  'blue': 918,\n",
       "  'dull': 919,\n",
       "  'folk': 920,\n",
       "  'letter': 921,\n",
       "  'whoever': 922,\n",
       "  'bond': 923,\n",
       "  'complaint': 924,\n",
       "  'expect': 925,\n",
       "  'scuffl': 926,\n",
       "  'carri': 927,\n",
       "  'hen': 928,\n",
       "  'end.': 929,\n",
       "  'congratul': 930,\n",
       "  'properti': 931,\n",
       "  'weaver': 932,\n",
       "  'poignantli': 933,\n",
       "  'spite': 934,\n",
       "  'burden': 935,\n",
       "  'cage': 936,\n",
       "  'passion': 937,\n",
       "  'print': 938,\n",
       "  'pirat': 939,\n",
       "  'worn': 940,\n",
       "  'clap': 941,\n",
       "  'prison': 942,\n",
       "  'understand': 943,\n",
       "  'sheer': 944,\n",
       "  'fieri': 945,\n",
       "  'palm-branch': 946,\n",
       "  'freedom': 947,\n",
       "  'fain': 948,\n",
       "  'hospit': 949,\n",
       "  'tut': 950,\n",
       "  'mind': 951,\n",
       "  'true': 952,\n",
       "  'think': 953,\n",
       "  'see': 954,\n",
       "  'unabl': 955,\n",
       "  'herostrat': 956,\n",
       "  'switzerland': 957,\n",
       "  'tin': 958,\n",
       "  'known': 959,\n",
       "  'lean': 960,\n",
       "  'lifeless': 961,\n",
       "  'bind': 962,\n",
       "  'inherit': 963,\n",
       "  'olive-grov': 964,\n",
       "  'sand': 965,\n",
       "  'remembr': 966,\n",
       "  'mountain': 967,\n",
       "  'lyric': 968,\n",
       "  'former': 969,\n",
       "  'pay': 970,\n",
       "  'reclus': 971,\n",
       "  'astonish': 972,\n",
       "  'child': 973,\n",
       "  'work': 974,\n",
       "  'artifici': 975,\n",
       "  'dead': 976,\n",
       "  'capit': 977,\n",
       "  'third': 978,\n",
       "  'leafi': 979,\n",
       "  'again.': 980,\n",
       "  'histori': 981,\n",
       "  'denot': 982,\n",
       "  'weari': 983,\n",
       "  'instantli': 984,\n",
       "  'insid': 985,\n",
       "  'univers': 986,\n",
       "  'play': 987,\n",
       "  'dutch': 988,\n",
       "  'mirror': 989,\n",
       "  'enter': 990,\n",
       "  'rose-bush': 991,\n",
       "  'aid': 992,\n",
       "  'bill': 993,\n",
       "  'cherry-tre': 994,\n",
       "  'bare': 995,\n",
       "  'famou': 996,\n",
       "  'joy': 997,\n",
       "  'queri': 998,\n",
       "  'friendli': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355a1a",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM Masked Language Modeling\n",
    "\n",
    "references: \n",
    "\n",
    "https://keras.io/examples/nlp/masked_language_modeling/#create-bert-model-pretraining-model-for-masked-language-modeling\n",
    "\n",
    "https://www.kaggle.com/code/ritvik1909/masked-language-modelling-rnn#Data-Preparation\n",
    "\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "questions:\n",
    "- should we split data by sentence instead of by fixed window size of 20?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51007c",
   "metadata": {},
   "source": [
    "### more data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "103b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to vectors\n",
    "vectorized_text = list(map(lambda x: vocabulary[x], train_data))\n",
    "vectorized_text = np.array(vectorized_text)\n",
    "\n",
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "vocabulary['[mask]'] = mask_id\n",
    "\n",
    "# split data into sequences of length 20\n",
    "vectorized_text_len = len(vectorized_text) - (len(vectorized_text) % 20)\n",
    "vectorized_text = vectorized_text[:vectorized_text_len]\n",
    "vectorized_text = np.reshape(vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c34a2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2253, 3282, 2465, ..., 1829, 3713, 3930],\n",
       "       [4029, 3383, 1800, ..., 2230, 2040, 3441],\n",
       "       [1829, 2465, 3398, ...,  875, 3383,  750],\n",
       "       ...,\n",
       "       [ 518, 1709, 3282, ...,  908, 1174,  875],\n",
       "       [1787,  908, 1174, ..., 3839,  214, 1523],\n",
       "       [2021, 1966,  518, ...,  628, 2073, 1419]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f04468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_one_input_label(sequence):\n",
    "    \n",
    "    # randomly choose one position in sequence to mask\n",
    "    mask = np.random.randint(low=0, high=20)\n",
    "    \n",
    "    # add mask to input\n",
    "    masked_input = [token if i != mask else mask_id for i, token in enumerate(sequence)]\n",
    "    \n",
    "    # set all values in label to -1(ignored by loss function) except the value at the masked position\n",
    "    label = [-1 if i!= mask else token for i, token in enumerate(sequence)]\n",
    "    return masked_input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "76fbd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get masked inputs and labels\n",
    "def get_masked_inputs_labels(text):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for seq in text:\n",
    "        x,y = mask_one_input_label(seq)\n",
    "        inputs.append(x)\n",
    "        labels.append(y)\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d8dcf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = get_masked_inputs_labels(vectorized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a73b",
   "metadata": {},
   "source": [
    "### bi-directional lstm model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9830fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masked language modeling class\n",
    "class LSTM_MLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, input_length):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "        : param vocab_size : The number of unique words in the data\n",
    "        : param hidden_size   : The size of your desired RNN\n",
    "        : param embed_size : The size of your latent embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_length = input_length\n",
    "\n",
    "        ## TODO: define your trainable variables and/or layers here. This should include an\n",
    "        ## embedding component, and any other variables/layers you require.\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size+1, output_dim=self.embed_size)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "        self.dense1 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
    "\n",
    "        # fully connected linear layers\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        You must use an embedding layer as the first layer of your network (i.e. tf.nn.embedding_lookup or tf.keras.layers.Embedding)\n",
    "        :param inputs: word ids of shape (batch_size, 2)\n",
    "        :return: logits: The batch element probabilities as a tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        x = inputs\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9c274c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 14s 72ms/step - loss: 7.0499\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 5.6662\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 5.3911\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 5.2313\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 5.0641\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 4.8635\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 4.6110\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 4.3779\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 4.1853\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.9469\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 3.7062\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.4725\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 3.2321\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.0094\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 2.8030\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 2.5666\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 2.3164\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 2.0705\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 7s 66ms/step - loss: 1.8239\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 1.5914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8abf84fdf0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_MLM(vocab_size, 64, 20)\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "# accuracy is not a good measure\n",
    "model.compile(loss=loss_metric, optimizer='adam')\n",
    "model.fit(x=inputs, y=labels, batch_size=20, epochs=20) \n",
    "# we do not need validation because our purpose is only to learn the patterns in our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4294ef",
   "metadata": {},
   "source": [
    "### get predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "79ca807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction: still in progress\n",
    "\n",
    "def get_predicted_probability(masked_sentence, target_word):\n",
    "    masked_sentence = masked_sentence.split(' ')\n",
    "    mask_loc = masked_sentence.index('[mask]')\n",
    "    target_id = vocabulary[target_word]\n",
    "    query_id = [vocabulary[q] for q in masked_sentence]\n",
    "\n",
    "    query_id = tf.expand_dims(query_id, axis=0)\n",
    "    pred = model(query_id)[:,mask_loc, target_id]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fd046776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '[mask] like beauti dress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b1eeb29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02857665], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "934b9680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01522405], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "305dd1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00066057], dtype=float32)>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dfe65af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.19514e-05], dtype=float32)>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "556080d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'evil old [mask]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cb73c75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.3034821e-05], dtype=float32)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e27cd75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.0170372e-05], dtype=float32)>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "469602ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.3883402e-05], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'pretti [mask]'\n",
    "get_predicted_probability(test_sentence, 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "11a696da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00011371], dtype=float32)>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'boy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64fc2d",
   "metadata": {},
   "source": [
    "### access embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0ef8cd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4127, 64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed693a",
   "metadata": {},
   "source": [
    "### testing lstm model on HW4 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f9ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/hw4_train.txt', \"r\")\n",
    "\n",
    "hw4_data = file.read()\n",
    "hw4_data = hw4_data.replace('\\n', ' ').split(' ')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c71b8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_vocabulary, hw4_vocab_size = get_vocab(hw4_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b88121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to vectors\n",
    "hw4_vectorized_text = list(map(lambda x: hw4_vocabulary[x], hw4_data))\n",
    "hw4_vectorized_text = np.array(hw4_vectorized_text)\n",
    "\n",
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "hw4_vocabulary['[mask]'] = mask_id\n",
    "\n",
    "# split data into sequences of length 20\n",
    "hw4_vectorized_text_len = len(hw4_vectorized_text) - (len(hw4_vectorized_text) % 20)\n",
    "hw4_vectorized_text = hw4_vectorized_text[:hw4_vectorized_text_len]\n",
    "hw4_vectorized_text = np.reshape(hw4_vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d5989da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_inputs, hw4_labels = get_masked_inputs_labels(hw4_vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0e7a53e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing model performance on hw4 data:\n",
    "# model = LSTM_MLM(hw4_vocab_size, 64, 20)\n",
    "# loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "# model.compile(loss=loss_metric, optimizer='adam')\n",
    "# model.fit(x=hw4_inputs, y=hw4_labels, batch_size=20, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696ff06",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aa8ac",
   "metadata": {},
   "source": [
    "references: \"Attention Is All You Need\" paper by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head self-attention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embed_dim % self.num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f12b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, input_length, embed_dim, num_heads, ff_dim, num_blocks, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size + 1, embed_dim)\n",
    "        self.pos_encoding = self.positional_encoding(input_length, embed_dim)\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_blocks)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def positional_encoding(self, input_length, embed_dim):\n",
    "        angles = 1 / (10000 ** (tf.range(0, embed_dim, 2, dtype=tf.float32) / embed_dim))\n",
    "        pos_encodings = tf.range(input_length, dtype=tf.float32)[:, tf.newaxis] * angles\n",
    "        pos_encodings = tf.concat([tf.sin(pos_encodings), tf.cos(pos_encodings)], axis=-1)\n",
    "        pos_encodings = pos_encodings[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encodings, tf.float32)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        seq_length = inputs.shape[1]\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32))\n",
    "        embeddings += self.pos_encoding[:, :seq_length, :]\n",
    "\n",
    "        x = self.dropout(embeddings, training=training)\n",
    "\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training)\n",
    "\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64138dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "input_length = 20\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_blocks = 4\n",
    "\n",
    "# Instantiate the model\n",
    "model_t = Transformer(vocab_size, input_length, embed_dim, num_heads, ff_dim, num_blocks)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, ignore_index=-1)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model_t.compile(loss=loss_metric, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_t.fit(x=inputs, y=labels, validation_split=0.1, batch_size=20, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
