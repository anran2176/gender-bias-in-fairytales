{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e45b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ad03035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/selinawang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/selinawang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c30d69",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "questions:\n",
    "- what to do with stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c181",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c08905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"THE EMPEROR'S NEW CLOTHES\\n\", '\\n', 'Many years ago, there was an Emperor, who was so excessively fond of\\n', 'new clothes, that he spent all his money in dress. He did not trouble\\n', 'himself in the least about his soldiers; nor did he care to go either to\\n', 'the theatre or the chase, except for the opportunities then afforded him\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('pg1597.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa27d9",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3e8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    data = \"\".join(line)\n",
    "    data = word_tokenize(data)\n",
    "    words = \" \".join(data)\n",
    "    lower_w = words.lower()\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = lower_w.split(\" \")\n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    output = []\n",
    "    for word in filtered_sentence:\n",
    "        output.append((ps.stem(word)))\n",
    "    final_out = \" \".join(output)\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c73d4",
   "metadata": {},
   "source": [
    "### get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "39342b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    data = data.replace('\\n', ' ').split(' ')\n",
    "    words = list(set(data))\n",
    "    vocabulary = {word:index for index, word in enumerate(words)}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    return vocabulary, vocab_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0ed439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(lines)\n",
    "vocabulary, vocab_size = get_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "021aeda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4119"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dfe02bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spoil': 0,\n",
       " 'torrent': 1,\n",
       " 'told': 2,\n",
       " 'inquir': 3,\n",
       " 'brass': 4,\n",
       " 'root': 5,\n",
       " 'rare': 6,\n",
       " 'float': 7,\n",
       " \"one'\": 8,\n",
       " 'bluish': 9,\n",
       " 'coach': 10,\n",
       " 'inward': 11,\n",
       " 'breast': 12,\n",
       " 'tuck': 13,\n",
       " 'five': 14,\n",
       " 'patrol': 15,\n",
       " 'bearer': 16,\n",
       " 'sad': 17,\n",
       " 'inexhaust': 18,\n",
       " 'large-s': 19,\n",
       " 'it.': 20,\n",
       " 'tend': 21,\n",
       " 'banner': 22,\n",
       " 'necessari': 23,\n",
       " 'leap': 24,\n",
       " 'me.': 25,\n",
       " 'oil': 26,\n",
       " 'vaudevil': 27,\n",
       " 'forgot': 28,\n",
       " 'peer': 29,\n",
       " 'griev': 30,\n",
       " 'heed': 31,\n",
       " 'wall': 32,\n",
       " 'thanke': 33,\n",
       " 'mirror': 34,\n",
       " 'nybod': 35,\n",
       " 'curios': 36,\n",
       " 'chamber': 37,\n",
       " 'shop-window': 38,\n",
       " '!': 39,\n",
       " 'granni': 40,\n",
       " 'reveal': 41,\n",
       " 'outspread': 42,\n",
       " 'pay': 43,\n",
       " 'gain': 44,\n",
       " 'now-a-day': 45,\n",
       " 'broke': 46,\n",
       " 'curl': 47,\n",
       " 'medicin': 48,\n",
       " 'reli': 49,\n",
       " 'end': 50,\n",
       " 'rain-wat': 51,\n",
       " 'pistol': 52,\n",
       " 'war': 53,\n",
       " 'uniform': 54,\n",
       " 'else.': 55,\n",
       " 'whisper': 56,\n",
       " 'inquisit': 57,\n",
       " 'dozen': 58,\n",
       " 'front': 59,\n",
       " 'last': 60,\n",
       " 'thin': 61,\n",
       " 'flush': 62,\n",
       " 'speak': 63,\n",
       " 'morgana': 64,\n",
       " 'forehead': 65,\n",
       " 'late': 66,\n",
       " 'debt.': 67,\n",
       " 'templ': 68,\n",
       " 'deepli': 69,\n",
       " 'punch': 70,\n",
       " 'bachelor': 71,\n",
       " 'altar': 72,\n",
       " 'princess': 73,\n",
       " 'add': 74,\n",
       " 'english': 75,\n",
       " 'diet': 76,\n",
       " 'skates.': 77,\n",
       " 'nod': 78,\n",
       " 'perhap': 79,\n",
       " 'grate': 80,\n",
       " 'unworthi': 81,\n",
       " 'burgher': 82,\n",
       " 'event': 83,\n",
       " 'chair': 84,\n",
       " 'majest': 85,\n",
       " 'anxiou': 86,\n",
       " 'daughter': 87,\n",
       " 'elfin': 88,\n",
       " 'uncommonli': 89,\n",
       " 'walk': 90,\n",
       " 'begin': 91,\n",
       " 'hind': 92,\n",
       " 'lackey': 93,\n",
       " 'loft': 94,\n",
       " 'darker': 95,\n",
       " 'nearest': 96,\n",
       " 'capit': 97,\n",
       " 'threw': 98,\n",
       " 'exhort': 99,\n",
       " 'viewing-room': 100,\n",
       " 'couch': 101,\n",
       " 'pic-nic': 102,\n",
       " 'spike': 103,\n",
       " \"'ti\": 104,\n",
       " 'content': 105,\n",
       " 'butter': 106,\n",
       " 'shoot': 107,\n",
       " 'influenc': 108,\n",
       " 'everyday': 109,\n",
       " 'etna': 110,\n",
       " 'awaken': 111,\n",
       " 'rain-drop': 112,\n",
       " 'outpost': 113,\n",
       " 'fieri': 114,\n",
       " 'buy': 115,\n",
       " 'hous': 116,\n",
       " 'disturb': 117,\n",
       " 'give': 118,\n",
       " 'greet': 119,\n",
       " 'thyself': 120,\n",
       " 'arch': 121,\n",
       " 'washing-tub': 122,\n",
       " 'alon': 123,\n",
       " 'occasion': 124,\n",
       " 'scanti': 125,\n",
       " \"merchant'\": 126,\n",
       " 'button-hol': 127,\n",
       " 'thorn-bush': 128,\n",
       " 'clever': 129,\n",
       " 'tempest': 130,\n",
       " 'rage': 131,\n",
       " 'youngest': 132,\n",
       " 'railway': 133,\n",
       " 'transplant': 134,\n",
       " 'steam-engin': 135,\n",
       " 'back': 136,\n",
       " 'wast': 137,\n",
       " 'gleam': 138,\n",
       " 'oh': 139,\n",
       " 'frederickshafen': 140,\n",
       " 'singular': 141,\n",
       " 'taught': 142,\n",
       " 'imposs': 143,\n",
       " 'excess': 144,\n",
       " 'quietli': 145,\n",
       " 'ny': 146,\n",
       " 'share': 147,\n",
       " 'instantan': 148,\n",
       " 'snowdrop': 149,\n",
       " 'sweetly-smel': 150,\n",
       " 'asperula': 151,\n",
       " 'complet': 152,\n",
       " 'pitch-black': 153,\n",
       " 'sunday': 154,\n",
       " 'vapor-bath': 155,\n",
       " 'ugh': 156,\n",
       " 'driven': 157,\n",
       " 'club': 158,\n",
       " 'beak': 159,\n",
       " 'fat': 160,\n",
       " 'aliv': 161,\n",
       " 'steadi': 162,\n",
       " 'womanhood': 163,\n",
       " 'inimit': 164,\n",
       " 'smile': 165,\n",
       " 'instead': 166,\n",
       " 'symbol': 167,\n",
       " 'danc': 168,\n",
       " 'passag': 169,\n",
       " 'geography-book': 170,\n",
       " 'woodroof': 171,\n",
       " 'improv': 172,\n",
       " 'fragranc': 173,\n",
       " 'here.': 174,\n",
       " 'pirat': 175,\n",
       " 'famou': 176,\n",
       " 'convinc': 177,\n",
       " 'ill.': 178,\n",
       " 'exhal': 179,\n",
       " 'flatli': 180,\n",
       " 'heavenli': 181,\n",
       " 'sun': 182,\n",
       " 'gloom': 183,\n",
       " 'tea-parti': 184,\n",
       " 'cower': 185,\n",
       " 'neat': 186,\n",
       " 'karen': 187,\n",
       " '*': 188,\n",
       " 'iii': 189,\n",
       " 'plenti': 190,\n",
       " 'worthi': 191,\n",
       " 'boaster': 192,\n",
       " 'magic': 193,\n",
       " 'lack': 194,\n",
       " 'flower-pot': 195,\n",
       " \"'and\": 196,\n",
       " 'flea': 197,\n",
       " 'bare': 198,\n",
       " 'brave': 199,\n",
       " \"saw'st\": 200,\n",
       " 'bush': 201,\n",
       " 'merci': 202,\n",
       " 'nought': 203,\n",
       " 'flesh': 204,\n",
       " 'linden': 205,\n",
       " 'chimney-corn': 206,\n",
       " 'creation': 207,\n",
       " 'astonishingli': 208,\n",
       " 'kay': 209,\n",
       " 'unfortun': 210,\n",
       " 'empti': 211,\n",
       " 'view': 212,\n",
       " 'detail': 213,\n",
       " 'strike': 214,\n",
       " 'easili': 215,\n",
       " 'splendidli': 216,\n",
       " 'isl': 217,\n",
       " 'lawn': 218,\n",
       " 'claret': 219,\n",
       " 'mighti': 220,\n",
       " 'innoc': 221,\n",
       " 'prepar': 222,\n",
       " 'suspicion': 223,\n",
       " 'brass-wir': 224,\n",
       " 'got': 225,\n",
       " 'udder': 226,\n",
       " 'treasur': 227,\n",
       " 'proprietor': 228,\n",
       " 'fond': 229,\n",
       " 'sword': 230,\n",
       " 'pillar': 231,\n",
       " 'precis': 232,\n",
       " 'maid': 233,\n",
       " 'tuk': 234,\n",
       " 'odour': 235,\n",
       " 'deer': 236,\n",
       " 'more.': 237,\n",
       " 'especi': 238,\n",
       " 'regiment': 239,\n",
       " 'clear': 240,\n",
       " 'smoke': 241,\n",
       " 'refreshing.': 242,\n",
       " 'waterpail': 243,\n",
       " 'wild-dron': 244,\n",
       " 'anecdot': 245,\n",
       " 'lean': 246,\n",
       " 'schnipp-schnapp-schnurre-basselurr': 247,\n",
       " 'wearer': 248,\n",
       " 'rat': 249,\n",
       " 'sunni': 250,\n",
       " 'card': 251,\n",
       " 'accordingli': 252,\n",
       " 'short': 253,\n",
       " 'grand-moth': 254,\n",
       " 'desper': 255,\n",
       " 'belli': 256,\n",
       " 'certainli': 257,\n",
       " 'harm': 258,\n",
       " 'stung': 259,\n",
       " 'render': 260,\n",
       " 'citizen': 261,\n",
       " 'roll': 262,\n",
       " 'troop': 263,\n",
       " 'laid': 264,\n",
       " 'mental': 265,\n",
       " 'materi': 266,\n",
       " 'myrtle-branch': 267,\n",
       " 'mile-broad': 268,\n",
       " 'good-look': 269,\n",
       " 'gutter': 270,\n",
       " 'suitabl': 271,\n",
       " 'hour': 272,\n",
       " 'slope': 273,\n",
       " 'spectat': 274,\n",
       " 'even': 275,\n",
       " 'heath': 276,\n",
       " 'bac': 277,\n",
       " 'weed': 278,\n",
       " 'involuntarili': 279,\n",
       " 'happen': 280,\n",
       " 'wert': 281,\n",
       " 'bell-ring': 282,\n",
       " 'spici': 283,\n",
       " 'jesu': 284,\n",
       " 'abl': 285,\n",
       " 'locu': 286,\n",
       " 'intend': 287,\n",
       " 'forefath': 288,\n",
       " 'fool': 289,\n",
       " 'began': 290,\n",
       " 'hackney-coach': 291,\n",
       " \"'i\": 292,\n",
       " 'porcupin': 293,\n",
       " 'coffee-tre': 294,\n",
       " 'closet': 295,\n",
       " 'reflect': 296,\n",
       " 'thousand': 297,\n",
       " 'fuss': 298,\n",
       " 'joy.': 299,\n",
       " 'picture-book': 300,\n",
       " 'hardli': 301,\n",
       " 'tickl': 302,\n",
       " 'spitzbergen.': 303,\n",
       " 'miserabili': 304,\n",
       " 'soro': 305,\n",
       " 'driest': 306,\n",
       " 'cut': 307,\n",
       " 'curiou': 308,\n",
       " 'engross': 309,\n",
       " 'sleek': 310,\n",
       " 'court': 311,\n",
       " 'bang': 312,\n",
       " 'hare': 313,\n",
       " 'hung': 314,\n",
       " 'groan': 315,\n",
       " 'compass': 316,\n",
       " 'aloud': 317,\n",
       " 'you.': 318,\n",
       " 'els': 319,\n",
       " 'mackintosh': 320,\n",
       " 'offer': 321,\n",
       " 'swarm': 322,\n",
       " 'bolt': 323,\n",
       " 'handmaiden': 324,\n",
       " 'height': 325,\n",
       " 'bark': 326,\n",
       " 'sang': 327,\n",
       " 'fetter': 328,\n",
       " 'mous': 329,\n",
       " 'heat': 330,\n",
       " 'baggesen': 331,\n",
       " 'climat': 332,\n",
       " 'rememb': 333,\n",
       " 'elabor': 334,\n",
       " 'sir': 335,\n",
       " 'creeping-pl': 336,\n",
       " 'shower': 337,\n",
       " 'awhil': 338,\n",
       " 'ladder': 339,\n",
       " 'wander': 340,\n",
       " 'hereupon': 341,\n",
       " 'augh': 342,\n",
       " 'well': 343,\n",
       " 'wouldst': 344,\n",
       " 'held': 345,\n",
       " 'gaze': 346,\n",
       " 'morn': 347,\n",
       " 'hasti': 348,\n",
       " 'splendor': 349,\n",
       " 'rub': 350,\n",
       " 'recover.': 351,\n",
       " 'flutter': 352,\n",
       " 'friend': 353,\n",
       " 'nowher': 354,\n",
       " 'yellow-color': 355,\n",
       " 'answer': 356,\n",
       " 'polish': 357,\n",
       " 'crept': 358,\n",
       " 'adorn': 359,\n",
       " 'blow': 360,\n",
       " 'snatch': 361,\n",
       " 'bull-dog': 362,\n",
       " 'fish': 363,\n",
       " 'numb': 364,\n",
       " 'exchang': 365,\n",
       " 'seen': 366,\n",
       " 'grander': 367,\n",
       " 'tick': 368,\n",
       " 'fever': 369,\n",
       " 'dowri': 370,\n",
       " 'review': 371,\n",
       " 'doubtless': 372,\n",
       " \"d'or\": 373,\n",
       " 'price': 374,\n",
       " 'pretend': 375,\n",
       " 'bay': 376,\n",
       " 'seeth': 377,\n",
       " 'hollow': 378,\n",
       " 'fine': 379,\n",
       " 'denmark.': 380,\n",
       " 'toy': 381,\n",
       " 'carriag': 382,\n",
       " 'onward': 383,\n",
       " 'defend': 384,\n",
       " 'limit': 385,\n",
       " 'therein': 386,\n",
       " 'relationship': 387,\n",
       " 'fantast': 388,\n",
       " 'reclus': 389,\n",
       " 'watering-plac': 390,\n",
       " 'virgin': 391,\n",
       " 'bend': 392,\n",
       " 'entri': 393,\n",
       " 'richly-attir': 394,\n",
       " 'sum': 395,\n",
       " 'skirt': 396,\n",
       " 'northern': 397,\n",
       " 'strang': 398,\n",
       " 'infanc': 399,\n",
       " 'capitally.': 400,\n",
       " 'elder-flow': 401,\n",
       " 'splendidly-dress': 402,\n",
       " 'mice': 403,\n",
       " 'gooseberri': 404,\n",
       " 'hold': 405,\n",
       " 'farm-hous': 406,\n",
       " 'fairi': 407,\n",
       " 'imit': 408,\n",
       " 'invitingli': 409,\n",
       " 'morass.': 410,\n",
       " 'narrow': 411,\n",
       " 'innkeep': 412,\n",
       " 'new': 413,\n",
       " 'recognis': 414,\n",
       " 'ding': 415,\n",
       " 'diminut': 416,\n",
       " 'dr.': 417,\n",
       " 'witti': 418,\n",
       " 'lone': 419,\n",
       " 'black': 420,\n",
       " 'outrid': 421,\n",
       " 'sin': 422,\n",
       " 'prime': 423,\n",
       " 'pinch': 424,\n",
       " 'spirit': 425,\n",
       " 'fragrant': 426,\n",
       " 'windowpan': 427,\n",
       " 'repent': 428,\n",
       " 'sentri': 429,\n",
       " 'rabbit': 430,\n",
       " 'polar': 431,\n",
       " 'farther': 432,\n",
       " 'exactli': 433,\n",
       " 'ourselves.': 434,\n",
       " 'question': 435,\n",
       " 'fellow': 436,\n",
       " 'wine': 437,\n",
       " 'crack': 438,\n",
       " 'tower': 439,\n",
       " '[': 440,\n",
       " 'crowd': 441,\n",
       " 'race-hors': 442,\n",
       " 'heaviest': 443,\n",
       " 'god': 444,\n",
       " 'spell': 445,\n",
       " 'tini': 446,\n",
       " 'ride': 447,\n",
       " 'noisi': 448,\n",
       " 'ugli': 449,\n",
       " 'summer-t': 450,\n",
       " 'acknowledg': 451,\n",
       " 'switzerland': 452,\n",
       " 'carpet': 453,\n",
       " 'tipto': 454,\n",
       " 'build': 455,\n",
       " 'sneez': 456,\n",
       " 'ape': 457,\n",
       " 'deliv': 458,\n",
       " 'freedom': 459,\n",
       " 'destin': 460,\n",
       " 'visitor': 461,\n",
       " 'brother': 462,\n",
       " 'preciou': 463,\n",
       " 'nobl': 464,\n",
       " 'past': 465,\n",
       " 'corner': 466,\n",
       " 'puzzl': 467,\n",
       " 'radiant': 468,\n",
       " 'race': 469,\n",
       " 'tiger-lili': 470,\n",
       " 'declar': 471,\n",
       " 'to-day': 472,\n",
       " 'ice': 473,\n",
       " 'fanci': 474,\n",
       " 'produc': 475,\n",
       " \"sang'st\": 476,\n",
       " 'next': 477,\n",
       " 'drove': 478,\n",
       " 'among': 479,\n",
       " 'assert': 480,\n",
       " 'sweetheart': 481,\n",
       " 'tulip': 482,\n",
       " 'handsom': 483,\n",
       " 'royster': 484,\n",
       " 'galosh': 485,\n",
       " 'genteel': 486,\n",
       " 'girl': 487,\n",
       " 'cock': 488,\n",
       " 'survey': 489,\n",
       " 'palm-branch': 490,\n",
       " 'ill-favor': 491,\n",
       " 'impostur': 492,\n",
       " 'spear': 493,\n",
       " '1801': 494,\n",
       " 'ist': 495,\n",
       " 'oneself': 496,\n",
       " 'glad': 497,\n",
       " 'bough': 498,\n",
       " 'weather-cock': 499,\n",
       " 'box': 500,\n",
       " 'sweet': 501,\n",
       " 'open': 502,\n",
       " 'develop': 503,\n",
       " 'ranunculu': 504,\n",
       " 'bell': 505,\n",
       " 'ago': 506,\n",
       " 'harmless': 507,\n",
       " 'fare': 508,\n",
       " 'articl': 509,\n",
       " 'friendship': 510,\n",
       " 'vi': 511,\n",
       " 'eas': 512,\n",
       " 'kick': 513,\n",
       " 'many-color': 514,\n",
       " \"'\": 515,\n",
       " 'adventur': 516,\n",
       " 'polli': 517,\n",
       " 'pursu': 518,\n",
       " 'flaminiu': 519,\n",
       " 'rosenburg': 520,\n",
       " 'h.': 521,\n",
       " 'haughti': 522,\n",
       " 'leave.': 523,\n",
       " \"'true\": 524,\n",
       " 'doubl': 525,\n",
       " 'read': 526,\n",
       " 'insati': 527,\n",
       " 'within': 528,\n",
       " 'consid': 529,\n",
       " 'happier': 530,\n",
       " 'frost-work': 531,\n",
       " 'bear-bal': 532,\n",
       " 'fli': 533,\n",
       " 'becom': 534,\n",
       " 'moon-stricken': 535,\n",
       " 'eagerli': 536,\n",
       " 'court-ladi': 537,\n",
       " 'everybodi': 538,\n",
       " 'spread': 539,\n",
       " 'cherri': 540,\n",
       " 'furthermor': 541,\n",
       " 'fill': 542,\n",
       " 'array': 543,\n",
       " 'deeper': 544,\n",
       " 'owe': 545,\n",
       " 'paroxysm': 546,\n",
       " 'taper': 547,\n",
       " 'fir': 548,\n",
       " 'door.': 549,\n",
       " 'olden': 550,\n",
       " 'deceiv': 551,\n",
       " 'purest': 552,\n",
       " 'tomorrow': 553,\n",
       " 'ham': 554,\n",
       " 'respons': 555,\n",
       " 'mortal': 556,\n",
       " 'cosmopolit': 557,\n",
       " 'display': 558,\n",
       " 'africa': 559,\n",
       " 'eternally-creak': 560,\n",
       " 'veloc': 561,\n",
       " 'flung': 562,\n",
       " 'deposit': 563,\n",
       " 'mistaken': 564,\n",
       " 'wept': 565,\n",
       " 'lifeless': 566,\n",
       " 'undeni': 567,\n",
       " 'fame': 568,\n",
       " 'birthday': 569,\n",
       " 'stair': 570,\n",
       " 'peculiar': 571,\n",
       " 'true': 572,\n",
       " 'unnecessari': 573,\n",
       " 'ado': 574,\n",
       " 'burden': 575,\n",
       " 'grand-fath': 576,\n",
       " 'still.': 577,\n",
       " 'poem': 578,\n",
       " 'repair': 579,\n",
       " 'twig': 580,\n",
       " 'tree': 581,\n",
       " 'canal': 582,\n",
       " 'frozen': 583,\n",
       " 'brink': 584,\n",
       " 'portal': 585,\n",
       " 'eat': 586,\n",
       " 'soapbubbl': 587,\n",
       " 'boot': 588,\n",
       " 'casket': 589,\n",
       " 'balcony-door': 590,\n",
       " 'hue': 591,\n",
       " 'hurrah': 592,\n",
       " 'hovel': 593,\n",
       " 'farth': 594,\n",
       " 'wiser': 595,\n",
       " 'greenland': 596,\n",
       " 'artific': 597,\n",
       " 'luxuriance.': 598,\n",
       " 'rapid': 599,\n",
       " 'conceiv': 600,\n",
       " 'push': 601,\n",
       " 'shrunk': 602,\n",
       " 'charm': 603,\n",
       " 'laugh': 604,\n",
       " 'expens': 605,\n",
       " 'fifty-two': 606,\n",
       " 'wither': 607,\n",
       " 'pat': 608,\n",
       " 'treat': 609,\n",
       " 'barehead': 610,\n",
       " 'subject': 611,\n",
       " 'crumbl': 612,\n",
       " 'incens': 613,\n",
       " 'inch': 614,\n",
       " 'heavier': 615,\n",
       " 'lady.': 616,\n",
       " 'advanc': 617,\n",
       " 'grown': 618,\n",
       " 'poetri': 619,\n",
       " 'odd': 620,\n",
       " 'frog': 621,\n",
       " 'lanc': 622,\n",
       " 'inquiringli': 623,\n",
       " 'enchant': 624,\n",
       " 'magdalena': 625,\n",
       " 'pillori': 626,\n",
       " 'smaller': 627,\n",
       " 'lamp': 628,\n",
       " 'keep': 629,\n",
       " 'handl': 630,\n",
       " 'cash': 631,\n",
       " 'enabl': 632,\n",
       " 'guard': 633,\n",
       " 'ruin': 634,\n",
       " 'cloak': 635,\n",
       " 'none': 636,\n",
       " 'theatr': 637,\n",
       " 'bed': 638,\n",
       " 'inspect': 639,\n",
       " 'essenti': 640,\n",
       " 'snow-flak': 641,\n",
       " 'dragon': 642,\n",
       " 'inherit': 643,\n",
       " 'bunch': 644,\n",
       " 'waltz': 645,\n",
       " 'hoax': 646,\n",
       " 'undergon': 647,\n",
       " 'skeleton': 648,\n",
       " 'passer-bi': 649,\n",
       " 'poet': 650,\n",
       " 'time': 651,\n",
       " 'witchcraft': 652,\n",
       " 'hearest': 653,\n",
       " 'distribut': 654,\n",
       " 'wondrous': 655,\n",
       " 'tour': 656,\n",
       " 'enough.': 657,\n",
       " 'stern': 658,\n",
       " 'calcul': 659,\n",
       " 'block': 660,\n",
       " 'knot': 661,\n",
       " 'allow': 662,\n",
       " 'summer-tim': 663,\n",
       " 'life': 664,\n",
       " 'dwell': 665,\n",
       " 'promis': 666,\n",
       " 'yearli': 667,\n",
       " 'firmli': 668,\n",
       " 'viz': 669,\n",
       " 'commiss': 670,\n",
       " 'elderbush': 671,\n",
       " 'millin': 672,\n",
       " 'directori': 673,\n",
       " 'employ': 674,\n",
       " 'walking-stick': 675,\n",
       " 'too.': 676,\n",
       " 'band': 677,\n",
       " 'figur': 678,\n",
       " 'gnaw': 679,\n",
       " 'conceal': 680,\n",
       " 'felt': 681,\n",
       " 'elder': 682,\n",
       " 'opposit': 683,\n",
       " 'twilight': 684,\n",
       " 'verili': 685,\n",
       " 'real': 686,\n",
       " 'cottag': 687,\n",
       " 'embodi': 688,\n",
       " 'phantast': 689,\n",
       " 'equipag': 690,\n",
       " 'cupid': 691,\n",
       " 'hop': 692,\n",
       " 'oil-skin': 693,\n",
       " 'prasto': 694,\n",
       " 'unusu': 695,\n",
       " 'magician': 696,\n",
       " 'holiday': 697,\n",
       " 'bestow': 698,\n",
       " 'drummer': 699,\n",
       " 'length': 700,\n",
       " 'wood': 701,\n",
       " 'unhind': 702,\n",
       " 'drift': 703,\n",
       " 'vexat': 704,\n",
       " 'royal': 705,\n",
       " 'ant-hil': 706,\n",
       " 'lofti': 707,\n",
       " 'reign': 708,\n",
       " 'distinct': 709,\n",
       " 'sew': 710,\n",
       " 'fabl': 711,\n",
       " 'honey-be': 712,\n",
       " 'kindli': 713,\n",
       " 'unti': 714,\n",
       " 'police-offic': 715,\n",
       " 'manor-hous': 716,\n",
       " 'willingli': 717,\n",
       " 'dramat': 718,\n",
       " 'head': 719,\n",
       " \"would'st\": 720,\n",
       " 'public.': 721,\n",
       " 'fix': 722,\n",
       " 'project': 723,\n",
       " 'rascal': 724,\n",
       " 'societi': 725,\n",
       " 'circumst': 726,\n",
       " 'offic': 727,\n",
       " 'richard': 728,\n",
       " 'poison': 729,\n",
       " 'hiss': 730,\n",
       " 'leant': 731,\n",
       " 'cavali': 732,\n",
       " 'wonder': 733,\n",
       " 'recal': 734,\n",
       " 'overpow': 735,\n",
       " 'weavers.': 736,\n",
       " 'possibl': 737,\n",
       " 'torment': 738,\n",
       " 'glass': 739,\n",
       " 'pleasant': 740,\n",
       " 'examined.': 741,\n",
       " 'lyric': 742,\n",
       " 'gay': 743,\n",
       " 'cri': 744,\n",
       " 'beer': 745,\n",
       " 'pseudo-herschel': 746,\n",
       " 'sting': 747,\n",
       " 'flower-garden': 748,\n",
       " '1482-1513': 749,\n",
       " 'awar': 750,\n",
       " 'twelv': 751,\n",
       " 'bran': 752,\n",
       " 'larg': 753,\n",
       " 'funer': 754,\n",
       " 'pet': 755,\n",
       " 'announc': 756,\n",
       " 'bent': 757,\n",
       " 'blew': 758,\n",
       " 'sunbeam': 759,\n",
       " 'page': 760,\n",
       " 'ii': 761,\n",
       " 'diana': 762,\n",
       " 'flap': 763,\n",
       " 'rheumat': 764,\n",
       " 'odor': 765,\n",
       " 'gibberish': 766,\n",
       " 'shiver': 767,\n",
       " 'silken': 768,\n",
       " 'south': 769,\n",
       " 'asund': 770,\n",
       " 'moulder': 771,\n",
       " 'joy': 772,\n",
       " 'bud': 773,\n",
       " 'bright': 774,\n",
       " 'spring': 775,\n",
       " 'execution.': 776,\n",
       " 'coo': 777,\n",
       " 'hoar-frost': 778,\n",
       " 'gloriou': 779,\n",
       " 'anxious': 780,\n",
       " 'toledo': 781,\n",
       " 'satin': 782,\n",
       " 'certain': 783,\n",
       " 'thick': 784,\n",
       " 'doll': 785,\n",
       " 'instantli': 786,\n",
       " 'extinct': 787,\n",
       " 'gigant': 788,\n",
       " 'sand': 789,\n",
       " 'narcissu': 790,\n",
       " 'mahogani': 791,\n",
       " 'hand.': 792,\n",
       " 'hotter': 793,\n",
       " 'bring': 794,\n",
       " 'whoever': 795,\n",
       " 'compani': 796,\n",
       " 'taken': 797,\n",
       " 'rabbl': 798,\n",
       " 'oppress': 799,\n",
       " 'sorri': 800,\n",
       " 'equal': 801,\n",
       " 'ill-humor': 802,\n",
       " 'fals': 803,\n",
       " 'whither': 804,\n",
       " 'wick': 805,\n",
       " 'whiter': 806,\n",
       " 'moment': 807,\n",
       " 'messeng': 808,\n",
       " 'draw': 809,\n",
       " 'search': 810,\n",
       " 'splinter': 811,\n",
       " 'solemnli': 812,\n",
       " 'young': 813,\n",
       " 'crafti': 814,\n",
       " 'wedding-day': 815,\n",
       " 'mute': 816,\n",
       " 'dutch': 817,\n",
       " 'distinctli': 818,\n",
       " 'construct': 819,\n",
       " 'heel': 820,\n",
       " 'bitter': 821,\n",
       " 'flower': 822,\n",
       " 'queen': 823,\n",
       " 'one': 824,\n",
       " 'wave': 825,\n",
       " \"god'\": 826,\n",
       " 'perplex': 827,\n",
       " 'stockingless': 828,\n",
       " 'gentl': 829,\n",
       " 'wood-pigeon': 830,\n",
       " 'staircas': 831,\n",
       " 'folding-door': 832,\n",
       " 'scrubbi': 833,\n",
       " 'antiqu': 834,\n",
       " 'moist': 835,\n",
       " 'pipe': 836,\n",
       " 'vesuviu': 837,\n",
       " 'bright-r': 838,\n",
       " 'comedi': 839,\n",
       " 'darl': 840,\n",
       " 'negro': 841,\n",
       " 'manufactur': 842,\n",
       " 'skil': 843,\n",
       " 'melodi': 844,\n",
       " 'shop': 845,\n",
       " 'canst': 846,\n",
       " 'string': 847,\n",
       " 'olive-grov': 848,\n",
       " 'fortune-tel': 849,\n",
       " 'bitten': 850,\n",
       " 'looking-glass': 851,\n",
       " 'bargain': 852,\n",
       " 'art': 853,\n",
       " 'period': 854,\n",
       " 'well-light': 855,\n",
       " 'mattress': 856,\n",
       " 'climb': 857,\n",
       " 'industri': 858,\n",
       " 'pride': 859,\n",
       " 'robe': 860,\n",
       " 'sink': 861,\n",
       " 'fulli': 862,\n",
       " 'sun-light': 863,\n",
       " 'strong': 864,\n",
       " 'submit': 865,\n",
       " 'attack': 866,\n",
       " 'knapsack': 867,\n",
       " 'dress': 868,\n",
       " 'condit': 869,\n",
       " 'bushi': 870,\n",
       " 'to.': 871,\n",
       " 'permit': 872,\n",
       " 'augusta': 873,\n",
       " 'splendid': 874,\n",
       " 'famili': 875,\n",
       " 'torch': 876,\n",
       " 'appleblossom': 877,\n",
       " 'milk': 878,\n",
       " 'foliag': 879,\n",
       " 'born': 880,\n",
       " 'coat': 881,\n",
       " 'pigeon': 882,\n",
       " 'soothsay': 883,\n",
       " 'nearer': 884,\n",
       " 'overshadow': 885,\n",
       " 'exhaust': 886,\n",
       " 'about.': 887,\n",
       " 'comfort': 888,\n",
       " 'twine': 889,\n",
       " 'boy': 890,\n",
       " 'tender': 891,\n",
       " 'destroy': 892,\n",
       " 'carri': 893,\n",
       " 'heart': 894,\n",
       " 'proud': 895,\n",
       " 'safe': 896,\n",
       " 'andersen': 897,\n",
       " 'decid': 898,\n",
       " 'godfrey': 899,\n",
       " 'toad': 900,\n",
       " 'terror': 901,\n",
       " 'duu': 902,\n",
       " 'regularli': 903,\n",
       " 'accomplish': 904,\n",
       " 'absorb': 905,\n",
       " 'church-bel': 906,\n",
       " 'graciou': 907,\n",
       " 'canopi': 908,\n",
       " 'congeni': 909,\n",
       " 'school.': 910,\n",
       " 'adopt': 911,\n",
       " 'roses.': 912,\n",
       " 'stupid': 913,\n",
       " 'servant': 914,\n",
       " 'sheep': 915,\n",
       " 'caw': 916,\n",
       " 'lend': 917,\n",
       " 'chimney': 918,\n",
       " 'stamina': 919,\n",
       " 'nichola': 920,\n",
       " 'wrung': 921,\n",
       " 'passport': 922,\n",
       " 'sundri': 923,\n",
       " 'violent': 924,\n",
       " 'sleep.': 925,\n",
       " 'set': 926,\n",
       " 'simplic': 927,\n",
       " 'shut': 928,\n",
       " 'breath': 929,\n",
       " 'squeez': 930,\n",
       " 'encourag': 931,\n",
       " 'revel': 932,\n",
       " 'discret': 933,\n",
       " 'mischiev': 934,\n",
       " 'grand': 935,\n",
       " 'cold': 936,\n",
       " 'sake': 937,\n",
       " 'mere': 938,\n",
       " 'unwont': 939,\n",
       " 'forev': 940,\n",
       " 'skate': 941,\n",
       " 'hors': 942,\n",
       " 'tumbl': 943,\n",
       " 'knee': 944,\n",
       " 'starve.': 945,\n",
       " 'approach': 946,\n",
       " 'grown-up-person': 947,\n",
       " 'hidden': 948,\n",
       " 'magnific': 949,\n",
       " 'tap': 950,\n",
       " 'ephemera': 951,\n",
       " 'hang': 952,\n",
       " 'bet': 953,\n",
       " 'correct': 954,\n",
       " 'caus': 955,\n",
       " 'hallelujah': 956,\n",
       " 'anoth': 957,\n",
       " 'trial': 958,\n",
       " 'agre': 959,\n",
       " 'snail': 960,\n",
       " 'show': 961,\n",
       " 'crew': 962,\n",
       " 'emul': 963,\n",
       " 'guest': 964,\n",
       " 'piou': 965,\n",
       " 'shopman': 966,\n",
       " 'duti': 967,\n",
       " 'kitchen': 968,\n",
       " 'marriag': 969,\n",
       " 'needl': 970,\n",
       " 'join': 971,\n",
       " 'quickli': 972,\n",
       " 'effectu': 973,\n",
       " 'unweari': 974,\n",
       " 'home': 975,\n",
       " 'rattl': 976,\n",
       " 'councillorship': 977,\n",
       " 'spit': 978,\n",
       " 'sheet': 979,\n",
       " 'fie': 980,\n",
       " 'avoid': 981,\n",
       " 'smooth': 982,\n",
       " 'snow.': 983,\n",
       " 'beast': 984,\n",
       " 'dissatisfact': 985,\n",
       " 'bodic': 986,\n",
       " 'hair-comb': 987,\n",
       " 'whimper': 988,\n",
       " 'prescrib': 989,\n",
       " 'row': 990,\n",
       " 'cup': 991,\n",
       " 'pull': 992,\n",
       " 'lop': 993,\n",
       " 'nay': 994,\n",
       " 'farthest': 995,\n",
       " 'printer': 996,\n",
       " 'seven': 997,\n",
       " 'ground': 998,\n",
       " 'misfortun': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355a1a",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM Masked Language Modeling\n",
    "\n",
    "references: \n",
    "\n",
    "https://keras.io/examples/nlp/masked_language_modeling/#create-bert-model-pretraining-model-for-masked-language-modeling\n",
    "\n",
    "https://www.kaggle.com/code/ritvik1909/masked-language-modelling-rnn#Data-Preparation\n",
    "\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "questions:\n",
    "- should we split data by sentence instead of by fixed window size of 20?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51007c",
   "metadata": {},
   "source": [
    "### more data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7d5b6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "vocabulary['[mask]'] = mask_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "103b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to vectors\n",
    "vectorized_text = list(map(lambda x: vocabulary[x], train_data))\n",
    "vectorized_text = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "37720b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into sequences of length 20\n",
    "vectorized_text_len = len(vectorized_text) - (len(vectorized_text) % 20)\n",
    "vectorized_text = vectorized_text[:vectorized_text_len]\n",
    "vectorized_text = np.reshape(vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c34a2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1567, 3934,  413, ...,  868, 2541, 2780],\n",
       "       [2465, 1168, 1045, ..., 1432,  272, 1297],\n",
       "       [1045, 3584, 1567, ..., 1567, 3330, 2471],\n",
       "       ...,\n",
       "       [3280, 2445, 1946, ..., 1739, 3885, 3205],\n",
       "       [1481, 2714, 3265, ..., 2701,  874, 3731],\n",
       "       [3577, 3205, 1542, ..., 2541, 1792,   32]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0f04468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_input_label(sequence):\n",
    "    \n",
    "    # randomly choose one position in sequence to mask\n",
    "    mask = np.random.randint(low=0, high=20)\n",
    "    \n",
    "    # add mask to input\n",
    "    masked_input = [token if i != mask else mask_id for i, token in enumerate(sequence)]\n",
    "    \n",
    "    # set all values in label to -1(ignored by loss function) except the value at the masked position\n",
    "    label = [-1 if i!= mask else token for i, token in enumerate(sequence)]\n",
    "    return masked_input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "76fbd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get masked inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for seq in vectorized_text:\n",
    "    x,y = get_masked_input_label(seq)\n",
    "    inputs.append(x)\n",
    "    labels.append(y)\n",
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b6cd526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1820, 20), (1820, 20))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "623bc977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1, 3330,   -1],\n",
       "       ...,\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1,   -1,   -1],\n",
       "       [  -1,   -1,   -1, ...,   -1, 1792,   -1]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a73b",
   "metadata": {},
   "source": [
    "### bi-directional lstm model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9830fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masked language modeling class\n",
    "class LSTM_MLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, input_length):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "        : param vocab_size : The number of unique words in the data\n",
    "        : param hidden_size   : The size of your desired RNN\n",
    "        : param embed_size : The size of your latent embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_length = input_length\n",
    "\n",
    "        ## TODO: define your trainable variables and/or layers here. This should include an\n",
    "        ## embedding component, and any other variables/layers you require.\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size+1, output_dim=self.embed_size)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "        self.dense1 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
    "\n",
    "        # fully connected linear layers\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        You must use an embedding layer as the first layer of your network (i.e. tf.nn.embedding_lookup or tf.keras.layers.Embedding)\n",
    "        :param inputs: word ids of shape (batch_size, 2)\n",
    "        :return: logits: The batch element probabilities as a tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        x = inputs\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c274c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "82/82 [==============================] - 13s 86ms/step - loss: 7.1600 - acc: 0.0049 - val_loss: 6.8625 - val_acc: 0.0052\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 5.6964 - acc: 0.0056 - val_loss: 7.1397 - val_acc: 0.0052\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 5.3775 - acc: 0.0056 - val_loss: 7.3514 - val_acc: 0.0052\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 5.1964 - acc: 0.0056 - val_loss: 8.0358 - val_acc: 0.0052\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 5.0402 - acc: 0.0056 - val_loss: 8.0269 - val_acc: 0.0052\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.8515 - acc: 0.0055 - val_loss: 8.3900 - val_acc: 0.0041\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.6261 - acc: 0.0057 - val_loss: 9.1050 - val_acc: 0.0025\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 4.3664 - acc: 0.0060 - val_loss: 9.4967 - val_acc: 0.0027\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 4.1173 - acc: 0.0068 - val_loss: 9.7108 - val_acc: 0.0025\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 3.8784 - acc: 0.0075 - val_loss: 10.2482 - val_acc: 0.0027\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 3.6494 - acc: 0.0083 - val_loss: 10.5441 - val_acc: 0.0025\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 3.4100 - acc: 0.0090 - val_loss: 10.8788 - val_acc: 0.0019\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 3.1634 - acc: 0.0107 - val_loss: 11.1392 - val_acc: 0.0022\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 2.9277 - acc: 0.0126 - val_loss: 11.4346 - val_acc: 0.0016\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 6s 76ms/step - loss: 2.7108 - acc: 0.0147 - val_loss: 11.8988 - val_acc: 0.0014\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 2.5048 - acc: 0.0174 - val_loss: 11.9514 - val_acc: 0.0019\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 2.2714 - acc: 0.0200 - val_loss: 12.4020 - val_acc: 0.0014\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 6s 74ms/step - loss: 2.0444 - acc: 0.0229 - val_loss: 12.6693 - val_acc: 0.0014\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 1.8405 - acc: 0.0268 - val_loss: 12.7901 - val_acc: 0.0014\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 1.6296 - acc: 0.0306 - val_loss: 12.9688 - val_acc: 0.0014\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 1.4437 - acc: 0.0340 - val_loss: 13.1367 - val_acc: 8.2418e-04\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 1.2877 - acc: 0.0354 - val_loss: 13.5050 - val_acc: 0.0011\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 1.1379 - acc: 0.0377 - val_loss: 13.7874 - val_acc: 0.0016\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.9905 - acc: 0.0402 - val_loss: 13.8969 - val_acc: 0.0014\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.9051 - acc: 0.0408 - val_loss: 14.1263 - val_acc: 0.0019\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.7965 - acc: 0.0421 - val_loss: 14.1428 - val_acc: 8.2418e-04\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.6773 - acc: 0.0439 - val_loss: 14.3249 - val_acc: 0.0011\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.5818 - acc: 0.0449 - val_loss: 14.5124 - val_acc: 0.0011\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.5101 - acc: 0.0459 - val_loss: 14.6368 - val_acc: 8.2418e-04\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.4727 - acc: 0.0459 - val_loss: 14.7187 - val_acc: 0.0014\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.4371 - acc: 0.0462 - val_loss: 14.9885 - val_acc: 0.0011\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.3822 - acc: 0.0468 - val_loss: 15.1323 - val_acc: 0.0011\n",
      "Epoch 33/50\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.3487 - acc: 0.0472 - val_loss: 15.2297 - val_acc: 8.2418e-04\n",
      "Epoch 34/50\n",
      "82/82 [==============================] - 6s 77ms/step - loss: 0.3026 - acc: 0.0476 - val_loss: 15.3850 - val_acc: 0.0016\n",
      "Epoch 35/50\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 0.2595 - acc: 0.0478 - val_loss: 15.5696 - val_acc: 0.0011\n",
      "Epoch 36/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.2414 - acc: 0.0482 - val_loss: 15.5395 - val_acc: 8.2418e-04\n",
      "Epoch 37/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.2098 - acc: 0.0484 - val_loss: 15.7407 - val_acc: 2.7473e-04\n",
      "Epoch 38/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.1816 - acc: 0.0488 - val_loss: 15.8790 - val_acc: 5.4945e-04\n",
      "Epoch 39/50\n",
      "82/82 [==============================] - 6s 76ms/step - loss: 0.1775 - acc: 0.0487 - val_loss: 15.8381 - val_acc: 8.2418e-04\n",
      "Epoch 40/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1646 - acc: 0.0489 - val_loss: 16.0646 - val_acc: 5.4945e-04\n",
      "Epoch 41/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1724 - acc: 0.0487 - val_loss: 16.1265 - val_acc: 8.2418e-04\n",
      "Epoch 42/50\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 0.1622 - acc: 0.0490 - val_loss: 16.0373 - val_acc: 5.4945e-04\n",
      "Epoch 43/50\n",
      "82/82 [==============================] - 6s 74ms/step - loss: 0.1368 - acc: 0.0491 - val_loss: 16.1991 - val_acc: 5.4945e-04\n",
      "Epoch 44/50\n",
      "82/82 [==============================] - 6s 72ms/step - loss: 0.1270 - acc: 0.0491 - val_loss: 16.3785 - val_acc: 2.7473e-04\n",
      "Epoch 45/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.1120 - acc: 0.0495 - val_loss: 16.3501 - val_acc: 2.7473e-04\n",
      "Epoch 46/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.1219 - acc: 0.0492 - val_loss: 16.5607 - val_acc: 2.7473e-04\n",
      "Epoch 47/50\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 0.0967 - acc: 0.0494 - val_loss: 16.5257 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0742 - acc: 0.0498 - val_loss: 16.6474 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0606 - acc: 0.0498 - val_loss: 16.7899 - val_acc: 2.7473e-04\n",
      "Epoch 50/50\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.0582 - acc: 0.0498 - val_loss: 16.8156 - val_acc: 5.4945e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd82f3788b0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_MLM(vocab_size, 64, 20)\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "model.compile(loss=loss_metric, optimizer='adam', metrics=['acc'])\n",
    "model.fit(x=inputs, y=labels, validation_split=0.1, batch_size=20, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "79ca807a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'embedding_13' (type Embedding).\n\n'list' object has no attribute 'dtype'\n\nCall arguments received by layer 'embedding_13' (type Embedding):\n  â€¢ inputs=['3577', '3205']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make prediction: still in progress\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#query = ['emperor', 'like', '[mask]', 'cloth', 'dress']\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#query_id = [vocabulary[q] for q in query]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query_id \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pred\n",
      "File \u001b[0;32m/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[116], line 38\u001b[0m, in \u001b[0;36mLSTM_MLM.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# embedding layer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'embedding_13' (type Embedding).\n\n'list' object has no attribute 'dtype'\n\nCall arguments received by layer 'embedding_13' (type Embedding):\n  â€¢ inputs=['3577', '3205']"
     ]
    }
   ],
   "source": [
    "# make prediction: still in progress\n",
    "\n",
    "#query = ['emperor', 'like', '[mask]', 'cloth', 'dress']\n",
    "#query_id = [vocabulary[q] for q in query]\n",
    "query_id = x[0:2]\n",
    "\n",
    "pred = model(query_id)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6969cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
