{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e45b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/apple/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /Users/apple/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/apple/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /Users/apple/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/apple/opt/anaconda3/envs/csci1470/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad03035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 23:19:13.936714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c30d69",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "- removed gender pronouns from list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c181",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c08905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"THE EMPEROR'S NEW CLOTHES\\n\", '\\n', 'Many years ago, there was an Emperor, who was so excessively fond of\\n', 'new clothes, that he spent all his money in dress. He did not trouble\\n', 'himself in the least about his soldiers; nor did he care to go either to\\n', 'the theatre or the chase, except for the opportunities then afforded him\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/pg1597.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa27d9",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    data = \"\".join(line)\n",
    "    data = word_tokenize(data)\n",
    "    words = \" \".join(data)\n",
    "    lower_w = words.lower()\n",
    "    \n",
    "    # keep gender pronouns in stop words\n",
    "    stop_words_keep = ['he',\n",
    "     'him',\n",
    "     'his',\n",
    "     'himself',\n",
    "     'she',\n",
    "     \"she's\",\n",
    "     'her',\n",
    "     'hers',\n",
    "     'herself']\n",
    "    stop_words = [word for word in stopwords.words('english') if word not in stop_words_keep]\n",
    "\n",
    "    stop_words = set(stop_words) \n",
    "    word_tokens = lower_w.split(\" \")\n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    output = []\n",
    "    for word in filtered_sentence:\n",
    "        output.append((ps.stem(word)))\n",
    "    final_out = \" \".join(output)\n",
    "    final_out = final_out.replace('\\n', ' ').split(' ')\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c73d4",
   "metadata": {},
   "source": [
    "### get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39342b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    words = list(set(data))\n",
    "    vocabulary = {word:index for index, word in enumerate(words)}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    return vocabulary, vocab_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(lines)\n",
    "vocabulary, vocab_size = get_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021aeda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4126,\n",
       " {'crowd': 0,\n",
       "  'grain': 1,\n",
       "  'still.': 2,\n",
       "  'opera': 3,\n",
       "  'candl': 4,\n",
       "  'escap': 5,\n",
       "  'glass.': 6,\n",
       "  '!': 7,\n",
       "  'tend': 8,\n",
       "  'unimport': 9,\n",
       "  'accept': 10,\n",
       "  'fragrant': 11,\n",
       "  'stork': 12,\n",
       "  'pronoun': 13,\n",
       "  'shabby.': 14,\n",
       "  'perpendicularli': 15,\n",
       "  'labor': 16,\n",
       "  'net': 17,\n",
       "  'dusti': 18,\n",
       "  'skeleton': 19,\n",
       "  'particular': 20,\n",
       "  'copper': 21,\n",
       "  'doubli': 22,\n",
       "  'tallow': 23,\n",
       "  'muff': 24,\n",
       "  'state': 25,\n",
       "  'map': 26,\n",
       "  'odor': 27,\n",
       "  'rosi': 28,\n",
       "  'heard': 29,\n",
       "  'porcupin': 30,\n",
       "  'ruin': 31,\n",
       "  'starched-up': 32,\n",
       "  'world-far': 33,\n",
       "  'suppl': 34,\n",
       "  'potato': 35,\n",
       "  'deceiv': 36,\n",
       "  'loveli': 37,\n",
       "  'popul': 38,\n",
       "  'door.': 39,\n",
       "  'unrol': 40,\n",
       "  'opposit': 41,\n",
       "  'rose-tre': 42,\n",
       "  'shoe': 43,\n",
       "  'cage': 44,\n",
       "  'leap': 45,\n",
       "  'doll': 46,\n",
       "  'angel': 47,\n",
       "  'fork': 48,\n",
       "  ',': 49,\n",
       "  'toll': 50,\n",
       "  'mind': 51,\n",
       "  'circumjac': 52,\n",
       "  'plum': 53,\n",
       "  'door-keep': 54,\n",
       "  'drove': 55,\n",
       "  'got': 56,\n",
       "  'insipid': 57,\n",
       "  'sentri': 58,\n",
       "  'enchant': 59,\n",
       "  'glow-worm': 60,\n",
       "  \"d'or\": 61,\n",
       "  'lili': 62,\n",
       "  'cours': 63,\n",
       "  'meadow': 64,\n",
       "  'can.': 65,\n",
       "  'ah': 66,\n",
       "  'apart': 67,\n",
       "  'mount': 68,\n",
       "  'trodden': 69,\n",
       "  'disappear': 70,\n",
       "  'eleg': 71,\n",
       "  'street': 72,\n",
       "  'princ': 73,\n",
       "  'within': 74,\n",
       "  'durat': 75,\n",
       "  'have.': 76,\n",
       "  'patienc': 77,\n",
       "  'prose': 78,\n",
       "  'worldli': 79,\n",
       "  ':': 80,\n",
       "  'unfett': 81,\n",
       "  'endow': 82,\n",
       "  'bought': 83,\n",
       "  'christianshafen': 84,\n",
       "  'behav': 85,\n",
       "  'sentinel': 86,\n",
       "  'loui': 87,\n",
       "  'nettl': 88,\n",
       "  'congratul': 89,\n",
       "  'purchas': 90,\n",
       "  'dark': 91,\n",
       "  'instrument.': 92,\n",
       "  'book': 93,\n",
       "  'higher': 94,\n",
       "  'sculptor': 95,\n",
       "  'face': 96,\n",
       "  'occupi': 97,\n",
       "  'town-gat': 98,\n",
       "  'progeni': 99,\n",
       "  'sink': 100,\n",
       "  'squeak': 101,\n",
       "  'gabl': 102,\n",
       "  'snow-flak': 103,\n",
       "  'tub': 104,\n",
       "  'good-by': 105,\n",
       "  'soup': 106,\n",
       "  'tickl': 107,\n",
       "  'alon': 108,\n",
       "  'ball-room': 109,\n",
       "  'crown': 110,\n",
       "  'well-cram': 111,\n",
       "  'baroni': 112,\n",
       "  'regiment': 113,\n",
       "  'waiting-maid': 114,\n",
       "  'impostur': 115,\n",
       "  'palm-branch': 116,\n",
       "  'recollect': 117,\n",
       "  'grand-daught': 118,\n",
       "  'known': 119,\n",
       "  'altar': 120,\n",
       "  'gnaw': 121,\n",
       "  'scaveng': 122,\n",
       "  'bade': 123,\n",
       "  'tone': 124,\n",
       "  'opinion': 125,\n",
       "  'form': 126,\n",
       "  'guard': 127,\n",
       "  'precaut': 128,\n",
       "  'citi': 129,\n",
       "  'languag': 130,\n",
       "  'sawn': 131,\n",
       "  'herbag': 132,\n",
       "  'train': 133,\n",
       "  'notwithstanding.': 134,\n",
       "  'voic': 135,\n",
       "  'lantern': 136,\n",
       "  'moonshin': 137,\n",
       "  'frame': 138,\n",
       "  'sight': 139,\n",
       "  'take': 140,\n",
       "  'sheet': 141,\n",
       "  'group': 142,\n",
       "  'fair': 143,\n",
       "  'nothing.': 144,\n",
       "  'leggin': 145,\n",
       "  'hallelujah': 146,\n",
       "  'vase': 147,\n",
       "  'milk': 148,\n",
       "  'board': 149,\n",
       "  'linen': 150,\n",
       "  'dutch': 151,\n",
       "  'inspect': 152,\n",
       "  'twice': 153,\n",
       "  'celebr': 154,\n",
       "  'viewing-room': 155,\n",
       "  'glide': 156,\n",
       "  'four': 157,\n",
       "  'wild-fowl': 158,\n",
       "  'drunk': 159,\n",
       "  'backward': 160,\n",
       "  'consid': 161,\n",
       "  'screen': 162,\n",
       "  'bearer': 163,\n",
       "  'famou': 164,\n",
       "  'happili': 165,\n",
       "  'umbrella': 166,\n",
       "  'discourag': 167,\n",
       "  'appleblossom': 168,\n",
       "  'porcelain': 169,\n",
       "  'wax-work': 170,\n",
       "  'tale.': 171,\n",
       "  'salad': 172,\n",
       "  'flower-box': 173,\n",
       "  'feel': 174,\n",
       "  'docendi': 175,\n",
       "  'still': 176,\n",
       "  'rook': 177,\n",
       "  'babel': 178,\n",
       "  'vi': 179,\n",
       "  'cut': 180,\n",
       "  'none': 181,\n",
       "  'golden': 182,\n",
       "  'complain': 183,\n",
       "  'half-dress': 184,\n",
       "  'bell-ring': 185,\n",
       "  'strip': 186,\n",
       "  'pet': 187,\n",
       "  'side': 188,\n",
       "  'superior': 189,\n",
       "  'romant': 190,\n",
       "  'shone': 191,\n",
       "  'europ': 192,\n",
       "  'dreari': 193,\n",
       "  'inexhaust': 194,\n",
       "  'pant': 195,\n",
       "  'red': 196,\n",
       "  'unabl': 197,\n",
       "  'foremost': 198,\n",
       "  'jew': 199,\n",
       "  'taught': 200,\n",
       "  'frighten': 201,\n",
       "  'bash': 202,\n",
       "  'steam-boat': 203,\n",
       "  'elaps': 204,\n",
       "  'snatch': 205,\n",
       "  'militari': 206,\n",
       "  'water-snak': 207,\n",
       "  'triangl': 208,\n",
       "  'trap': 209,\n",
       "  'road-sid': 210,\n",
       "  'skill': 211,\n",
       "  'courteous': 212,\n",
       "  'juic': 213,\n",
       "  'length': 214,\n",
       "  'mountain-path': 215,\n",
       "  'strength': 216,\n",
       "  'mile-broad': 217,\n",
       "  'cool': 218,\n",
       "  'trick': 219,\n",
       "  'smell': 220,\n",
       "  'diet': 221,\n",
       "  'bremen': 222,\n",
       "  'fool': 223,\n",
       "  'gerda': 224,\n",
       "  'strong-stem': 225,\n",
       "  'lay': 226,\n",
       "  'electr': 227,\n",
       "  'lighten': 228,\n",
       "  'suddenli': 229,\n",
       "  'gloriou': 230,\n",
       "  'touchingli': 231,\n",
       "  'cross-bow': 232,\n",
       "  'fright': 233,\n",
       "  'similar': 234,\n",
       "  'paint': 235,\n",
       "  'gaili': 236,\n",
       "  'barren': 237,\n",
       "  'swarm': 238,\n",
       "  'happen': 239,\n",
       "  'mice': 240,\n",
       "  'crocu': 241,\n",
       "  'realm': 242,\n",
       "  'every-day': 243,\n",
       "  'thither': 244,\n",
       "  'younger': 245,\n",
       "  'grass-plot': 246,\n",
       "  'hear': 247,\n",
       "  'buoyantli': 248,\n",
       "  'comprehend': 249,\n",
       "  'mistak': 250,\n",
       "  'frozen': 251,\n",
       "  'ding': 252,\n",
       "  'narrow': 253,\n",
       "  'stroke': 254,\n",
       "  'unobserv': 255,\n",
       "  'all': 256,\n",
       "  'accomplish': 257,\n",
       "  'peculiar': 258,\n",
       "  'sat': 259,\n",
       "  'receiv': 260,\n",
       "  'hoax': 261,\n",
       "  'church-bel': 262,\n",
       "  'chandeli': 263,\n",
       "  'gem': 264,\n",
       "  'home': 265,\n",
       "  'piec': 266,\n",
       "  'green': 267,\n",
       "  'etc': 268,\n",
       "  'grander': 269,\n",
       "  'us.': 270,\n",
       "  'freed': 271,\n",
       "  'distinct': 272,\n",
       "  'scanti': 273,\n",
       "  'grand-fath': 274,\n",
       "  'rid': 275,\n",
       "  'timber': 276,\n",
       "  'match': 277,\n",
       "  'six': 278,\n",
       "  'pigeon': 279,\n",
       "  'bishop': 280,\n",
       "  'brave': 281,\n",
       "  'quietli': 282,\n",
       "  'half': 283,\n",
       "  'ape': 284,\n",
       "  'noth': 285,\n",
       "  'ugli': 286,\n",
       "  'fragranc': 287,\n",
       "  'invis': 288,\n",
       "  'reckon': 289,\n",
       "  'cosmopolit': 290,\n",
       "  'someth': 291,\n",
       "  'bower': 292,\n",
       "  'luxuriance.': 293,\n",
       "  'pursu': 294,\n",
       "  'probabl': 295,\n",
       "  'scald': 296,\n",
       "  'game': 297,\n",
       "  'memori': 298,\n",
       "  'race-hors': 299,\n",
       "  'drum': 300,\n",
       "  'frederickshafen': 301,\n",
       "  '1484': 302,\n",
       "  'trunk': 303,\n",
       "  'need': 304,\n",
       "  'sun-ray': 305,\n",
       "  'men-folks.': 306,\n",
       "  'modern': 307,\n",
       "  'crew': 308,\n",
       "  'resembl': 309,\n",
       "  'worn': 310,\n",
       "  'shut': 311,\n",
       "  'pipe': 312,\n",
       "  'distress': 313,\n",
       "  'hottest': 314,\n",
       "  'secur': 315,\n",
       "  'casket': 316,\n",
       "  'regard': 317,\n",
       "  'brain': 318,\n",
       "  'torn': 319,\n",
       "  'benefit': 320,\n",
       "  'grown-up': 321,\n",
       "  'statu': 322,\n",
       "  'yonder': 323,\n",
       "  'poem': 324,\n",
       "  'flower': 325,\n",
       "  'duodecimo': 326,\n",
       "  'pop': 327,\n",
       "  'nobl': 328,\n",
       "  'shrivel': 329,\n",
       "  'fetter': 330,\n",
       "  'skate': 331,\n",
       "  'front': 332,\n",
       "  'thanke': 333,\n",
       "  'forbidden': 334,\n",
       "  'costli': 335,\n",
       "  'morgana': 336,\n",
       "  'cheer': 337,\n",
       "  'suffici': 338,\n",
       "  'kick': 339,\n",
       "  'posit': 340,\n",
       "  'shake': 341,\n",
       "  'fieri': 342,\n",
       "  'notion': 343,\n",
       "  'mark': 344,\n",
       "  'boil': 345,\n",
       "  'printer': 346,\n",
       "  'contentedli': 347,\n",
       "  'bank.': 348,\n",
       "  'cup': 349,\n",
       "  'lad': 350,\n",
       "  'tea-parti': 351,\n",
       "  'wretch': 352,\n",
       "  'plain': 353,\n",
       "  'egyptian': 354,\n",
       "  'numer': 355,\n",
       "  'count': 356,\n",
       "  'mutter': 357,\n",
       "  'devoutli': 358,\n",
       "  'bordingborg': 359,\n",
       "  'credit': 360,\n",
       "  'temper': 361,\n",
       "  'circl': 362,\n",
       "  'sweetli': 363,\n",
       "  'bum': 364,\n",
       "  'gleam': 365,\n",
       "  'bound': 366,\n",
       "  'hearest': 367,\n",
       "  'multipl': 368,\n",
       "  'weari': 369,\n",
       "  'daylight': 370,\n",
       "  'diamond': 371,\n",
       "  'vapor': 372,\n",
       "  'featur': 373,\n",
       "  'tenement': 374,\n",
       "  'simpleton': 375,\n",
       "  'weather': 376,\n",
       "  'latin': 377,\n",
       "  \"'thou\": 378,\n",
       "  'loosen': 379,\n",
       "  'gibberish': 380,\n",
       "  'clothespress': 381,\n",
       "  'bazar': 382,\n",
       "  'violet': 383,\n",
       "  'went': 384,\n",
       "  'ride': 385,\n",
       "  'therefor': 386,\n",
       "  'knife': 387,\n",
       "  'box': 388,\n",
       "  'curiou': 389,\n",
       "  'pendulum': 390,\n",
       "  'tender': 391,\n",
       "  'mould': 392,\n",
       "  'pillar': 393,\n",
       "  'corn': 394,\n",
       "  'darkest': 395,\n",
       "  'verbal': 396,\n",
       "  'taper': 397,\n",
       "  ')': 398,\n",
       "  'pitch-black': 399,\n",
       "  'discharg': 400,\n",
       "  'ass': 401,\n",
       "  'almanac': 402,\n",
       "  'thousand': 403,\n",
       "  'mahogani': 404,\n",
       "  'tire': 405,\n",
       "  'impress': 406,\n",
       "  'becom': 407,\n",
       "  'christmas-ev': 408,\n",
       "  'shallow': 409,\n",
       "  'equal': 410,\n",
       "  'chain': 411,\n",
       "  'pleasant': 412,\n",
       "  'laudatori': 413,\n",
       "  'salmon': 414,\n",
       "  'pancak': 415,\n",
       "  'helm': 416,\n",
       "  'suppli': 417,\n",
       "  'unperceiv': 418,\n",
       "  'wrong': 419,\n",
       "  'sailor': 420,\n",
       "  'effect': 421,\n",
       "  \"'d\": 422,\n",
       "  'church': 423,\n",
       "  'thine': 424,\n",
       "  'ascend': 425,\n",
       "  'room': 426,\n",
       "  'shape': 427,\n",
       "  'dream': 428,\n",
       "  'pillori': 429,\n",
       "  'canker': 430,\n",
       "  'inferior': 431,\n",
       "  'weg': 432,\n",
       "  'gratul': 433,\n",
       "  'wisest': 434,\n",
       "  'send': 435,\n",
       "  'novel': 436,\n",
       "  '?': 437,\n",
       "  'lent': 438,\n",
       "  'rancid': 439,\n",
       "  'patrol': 440,\n",
       "  'invitingli': 441,\n",
       "  'enough': 442,\n",
       "  'passion': 443,\n",
       "  'galosh': 444,\n",
       "  'individu': 445,\n",
       "  \"'the\": 446,\n",
       "  'spring-tim': 447,\n",
       "  'sneez': 448,\n",
       "  'jump': 449,\n",
       "  'gentli': 450,\n",
       "  'thrust': 451,\n",
       "  'besid': 452,\n",
       "  'buccan': 453,\n",
       "  'vice': 454,\n",
       "  'educ': 455,\n",
       "  'harm': 456,\n",
       "  'discret': 457,\n",
       "  'afford': 458,\n",
       "  'clear-sight': 459,\n",
       "  'grief': 460,\n",
       "  'trickl': 461,\n",
       "  'promis': 462,\n",
       "  'eternally-creak': 463,\n",
       "  'frost': 464,\n",
       "  'tuft': 465,\n",
       "  'friedericksberg': 466,\n",
       "  'epistl': 467,\n",
       "  'desk.': 468,\n",
       "  'tiresom': 469,\n",
       "  'hurl': 470,\n",
       "  'name': 471,\n",
       "  'glassi': 472,\n",
       "  \"'to\": 473,\n",
       "  'pirat': 474,\n",
       "  'extend': 475,\n",
       "  'crab': 476,\n",
       "  'these.': 477,\n",
       "  'quirre-vit': 478,\n",
       "  'calm': 479,\n",
       "  'shook': 480,\n",
       "  'tore': 481,\n",
       "  'maiden': 482,\n",
       "  'instantan': 483,\n",
       "  'danger': 484,\n",
       "  'minist': 485,\n",
       "  'ordain': 486,\n",
       "  'foliag': 487,\n",
       "  'openli': 488,\n",
       "  'halberdi': 489,\n",
       "  'shadowi': 490,\n",
       "  'mirth': 491,\n",
       "  'imit': 492,\n",
       "  'bear-bal': 493,\n",
       "  'eloqu': 494,\n",
       "  'nonsens': 495,\n",
       "  'kingdom': 496,\n",
       "  'longer': 497,\n",
       "  'race': 498,\n",
       "  'clap': 499,\n",
       "  'violenc': 500,\n",
       "  'willow': 501,\n",
       "  'hears': 502,\n",
       "  'shiver': 503,\n",
       "  'snow-whit': 504,\n",
       "  'winter': 505,\n",
       "  'fairi': 506,\n",
       "  'situat': 507,\n",
       "  'hair-comb.': 508,\n",
       "  'else.': 509,\n",
       "  'berri': 510,\n",
       "  'earnestli': 511,\n",
       "  'clock': 512,\n",
       "  'turn': 513,\n",
       "  'jutland.': 514,\n",
       "  'happiest': 515,\n",
       "  'becam': 516,\n",
       "  'larger': 517,\n",
       "  'ball-dress': 518,\n",
       "  'minut': 519,\n",
       "  'bewild': 520,\n",
       "  'sweetheart': 521,\n",
       "  \"hog's-leath\": 522,\n",
       "  'instinct': 523,\n",
       "  'stair': 524,\n",
       "  'stove': 525,\n",
       "  'suffer': 526,\n",
       "  'reason': 527,\n",
       "  'decreas': 528,\n",
       "  'promin': 529,\n",
       "  'transit': 530,\n",
       "  'imploringli': 531,\n",
       "  'times.': 532,\n",
       "  'stout': 533,\n",
       "  'meant': 534,\n",
       "  'excellenza': 535,\n",
       "  'river': 536,\n",
       "  'bottl': 537,\n",
       "  'permiss': 538,\n",
       "  'damsel': 539,\n",
       "  'clay-floor': 540,\n",
       "  'preciou': 541,\n",
       "  'hog': 542,\n",
       "  'sheep': 543,\n",
       "  'rough': 544,\n",
       "  'peoni': 545,\n",
       "  'address': 546,\n",
       "  'cottag': 547,\n",
       "  'say': 548,\n",
       "  'insinu': 549,\n",
       "  'vanquish': 550,\n",
       "  'storm': 551,\n",
       "  'axe': 552,\n",
       "  'pass': 553,\n",
       "  'among': 554,\n",
       "  'sympathi': 555,\n",
       "  'slept': 556,\n",
       "  'partner': 557,\n",
       "  'condescens': 558,\n",
       "  'unhind': 559,\n",
       "  'depict': 560,\n",
       "  'ici': 561,\n",
       "  'unintellig': 562,\n",
       "  'coverlet': 563,\n",
       "  'forget': 564,\n",
       "  'seal': 565,\n",
       "  'dithyramb': 566,\n",
       "  'pepper': 567,\n",
       "  'newer': 568,\n",
       "  'french': 569,\n",
       "  \"'ti\": 570,\n",
       "  'emul': 571,\n",
       "  'longer.': 572,\n",
       "  'flask': 573,\n",
       "  'gone': 574,\n",
       "  'case': 575,\n",
       "  'dawn': 576,\n",
       "  'behind': 577,\n",
       "  'ingredi': 578,\n",
       "  'burdock': 579,\n",
       "  'everywher': 580,\n",
       "  'can': 581,\n",
       "  'arrow': 582,\n",
       "  'sit': 583,\n",
       "  'semblanc': 584,\n",
       "  'execution.': 585,\n",
       "  'transport': 586,\n",
       "  'quiet': 587,\n",
       "  'light': 588,\n",
       "  'proprietor': 589,\n",
       "  'secu': 590,\n",
       "  'pervad': 591,\n",
       "  'wild': 592,\n",
       "  'properli': 593,\n",
       "  'hidden': 594,\n",
       "  'vesuviu': 595,\n",
       "  'trivial': 596,\n",
       "  'make': 597,\n",
       "  'scold': 598,\n",
       "  'ill-favor': 599,\n",
       "  'histori': 600,\n",
       "  'bore': 601,\n",
       "  'deliv': 602,\n",
       "  'palm-tre': 603,\n",
       "  'brambl': 604,\n",
       "  'templ': 605,\n",
       "  'way': 606,\n",
       "  'boot': 607,\n",
       "  'metropoli': 608,\n",
       "  'age.': 609,\n",
       "  'hammer': 610,\n",
       "  'alreadi': 611,\n",
       "  'palac': 612,\n",
       "  'comfort': 613,\n",
       "  'ought': 614,\n",
       "  'grumbl': 615,\n",
       "  'velvet': 616,\n",
       "  'grave': 617,\n",
       "  'flesh': 618,\n",
       "  'abyss': 619,\n",
       "  'transpar': 620,\n",
       "  'congeni': 621,\n",
       "  'spring': 622,\n",
       "  'hop': 623,\n",
       "  'puddl': 624,\n",
       "  'spoke': 625,\n",
       "  'fife': 626,\n",
       "  \"'ve\": 627,\n",
       "  'overlook': 628,\n",
       "  'splendor': 629,\n",
       "  'princip': 630,\n",
       "  'desert': 631,\n",
       "  'greek': 632,\n",
       "  'astonishingli': 633,\n",
       "  'ill-behav': 634,\n",
       "  'sweetly-smel': 635,\n",
       "  'rail': 636,\n",
       "  'lain': 637,\n",
       "  'gout': 638,\n",
       "  'iffven': 639,\n",
       "  'either': 640,\n",
       "  'stream': 641,\n",
       "  'gurgl': 642,\n",
       "  'nightingal': 643,\n",
       "  'soothsay': 644,\n",
       "  'inhabit': 645,\n",
       "  'h.': 646,\n",
       "  'recov': 647,\n",
       "  'unless': 648,\n",
       "  'tipsi': 649,\n",
       "  'various': 650,\n",
       "  'suitabl': 651,\n",
       "  'middl': 652,\n",
       "  'christmas-tre': 653,\n",
       "  'correctli': 654,\n",
       "  'silenc': 655,\n",
       "  'ran': 656,\n",
       "  'thinner': 657,\n",
       "  'rebelli': 658,\n",
       "  'rare': 659,\n",
       "  'swine': 660,\n",
       "  'return': 661,\n",
       "  'friendship': 662,\n",
       "  'frederick': 663,\n",
       "  'belief': 664,\n",
       "  'geographi': 665,\n",
       "  'prepar': 666,\n",
       "  'circumspectli': 667,\n",
       "  'cutlet': 668,\n",
       "  'tell': 669,\n",
       "  'window-shutt': 670,\n",
       "  'lofti': 671,\n",
       "  'limb': 672,\n",
       "  'stretch': 673,\n",
       "  'hang': 674,\n",
       "  \"king'\": 675,\n",
       "  'pillow': 676,\n",
       "  'councillor': 677,\n",
       "  'sunni': 678,\n",
       "  'better.': 679,\n",
       "  \"god'\": 680,\n",
       "  'couch': 681,\n",
       "  'dew': 682,\n",
       "  'coffin': 683,\n",
       "  'sung': 684,\n",
       "  'bedchamb': 685,\n",
       "  'fingerless': 686,\n",
       "  'project': 687,\n",
       "  'broker': 688,\n",
       "  'experienc': 689,\n",
       "  'bind': 690,\n",
       "  'barg': 691,\n",
       "  'brink': 692,\n",
       "  'impregn': 693,\n",
       "  'dock-leav': 694,\n",
       "  'shoes.': 695,\n",
       "  'right': 696,\n",
       "  'needl': 697,\n",
       "  'chivalr': 698,\n",
       "  'immedi': 699,\n",
       "  'bolt': 700,\n",
       "  'subterranean': 701,\n",
       "  'thu': 702,\n",
       "  'tranc': 703,\n",
       "  'directly.': 704,\n",
       "  'dancer': 705,\n",
       "  'wildli': 706,\n",
       "  'court-card': 707,\n",
       "  \"they'll\": 708,\n",
       "  'cloth': 709,\n",
       "  'chink': 710,\n",
       "  'construct': 711,\n",
       "  'distinguish': 712,\n",
       "  'even': 713,\n",
       "  'cook': 714,\n",
       "  \"polly'\": 715,\n",
       "  'ant-hil': 716,\n",
       "  'consider': 717,\n",
       "  'fare': 718,\n",
       "  'cloak': 719,\n",
       "  'privi': 720,\n",
       "  'beneath': 721,\n",
       "  'post-boy': 722,\n",
       "  'prevent': 723,\n",
       "  'bend': 724,\n",
       "  'hastili': 725,\n",
       "  'sunbeam': 726,\n",
       "  'mistaken': 727,\n",
       "  'amus': 728,\n",
       "  'branch': 729,\n",
       "  'fill': 730,\n",
       "  'professor': 731,\n",
       "  'haycock': 732,\n",
       "  'rich': 733,\n",
       "  'roof': 734,\n",
       "  'gild': 735,\n",
       "  'ringstead': 736,\n",
       "  'unanim': 737,\n",
       "  'destroy': 738,\n",
       "  'peal': 739,\n",
       "  'intox': 740,\n",
       "  'egypt': 741,\n",
       "  'recount': 742,\n",
       "  'morass.': 743,\n",
       "  'necess': 744,\n",
       "  'sixteen': 745,\n",
       "  'parsonag': 746,\n",
       "  'moulder': 747,\n",
       "  'wanton': 748,\n",
       "  'hardli': 749,\n",
       "  'proudli': 750,\n",
       "  'heathen': 751,\n",
       "  'disagre': 752,\n",
       "  'paradis': 753,\n",
       "  'suit': 754,\n",
       "  'chastis': 755,\n",
       "  '‚Äù': 756,\n",
       "  'wittic': 757,\n",
       "  'north': 758,\n",
       "  'neck': 759,\n",
       "  'meagr': 760,\n",
       "  'hovel': 761,\n",
       "  'pair': 762,\n",
       "  'review': 763,\n",
       "  'grandeur': 764,\n",
       "  'suspicion': 765,\n",
       "  'asund': 766,\n",
       "  'glitter': 767,\n",
       "  'pure': 768,\n",
       "  'darl': 769,\n",
       "  'wedding-day': 770,\n",
       "  'earth': 771,\n",
       "  'entri': 772,\n",
       "  'stump': 773,\n",
       "  'wound': 774,\n",
       "  'finer': 775,\n",
       "  'rise': 776,\n",
       "  'din': 777,\n",
       "  'stiff': 778,\n",
       "  'busi': 779,\n",
       "  'liveri': 780,\n",
       "  'drawer': 781,\n",
       "  'term': 782,\n",
       "  'extraordinarili': 783,\n",
       "  'deform': 784,\n",
       "  'eaten': 785,\n",
       "  'wrung': 786,\n",
       "  'unwel': 787,\n",
       "  'smelt': 788,\n",
       "  'block': 789,\n",
       "  'thereon': 790,\n",
       "  'could': 791,\n",
       "  'sovereign': 792,\n",
       "  'burden': 793,\n",
       "  'ventur': 794,\n",
       "  'sheer': 795,\n",
       "  'ranunculus': 796,\n",
       "  'hebrew': 797,\n",
       "  'allot': 798,\n",
       "  \"'s\": 799,\n",
       "  'hand.': 800,\n",
       "  'well-meant': 801,\n",
       "  'practis': 802,\n",
       "  'peopl': 803,\n",
       "  'cavern': 804,\n",
       "  'march': 805,\n",
       "  'gauz': 806,\n",
       "  'trace': 807,\n",
       "  'render': 808,\n",
       "  'palm': 809,\n",
       "  'shadow-skul': 810,\n",
       "  \"n't\": 811,\n",
       "  'array': 812,\n",
       "  'ground': 813,\n",
       "  'theatr': 814,\n",
       "  'condemn': 815,\n",
       "  'satin': 816,\n",
       "  'dryad': 817,\n",
       "  'tendril': 818,\n",
       "  'boaster': 819,\n",
       "  'burnt-out': 820,\n",
       "  'sober': 821,\n",
       "  'freak': 822,\n",
       "  'insignific': 823,\n",
       "  'attun': 824,\n",
       "  'disturb': 825,\n",
       "  'sleev': 826,\n",
       "  'summer': 827,\n",
       "  'lady.': 828,\n",
       "  'swineherd.': 829,\n",
       "  'chamberlain': 830,\n",
       "  'rank': 831,\n",
       "  'forehead': 832,\n",
       "  'bid': 833,\n",
       "  'speak.': 834,\n",
       "  'circumspect': 835,\n",
       "  'gain': 836,\n",
       "  'penetr': 837,\n",
       "  'voyag': 838,\n",
       "  'unweari': 839,\n",
       "  'bestow': 840,\n",
       "  'invent': 841,\n",
       "  'base': 842,\n",
       "  'nowher': 843,\n",
       "  'wive': 844,\n",
       "  'ladi': 845,\n",
       "  'pea': 846,\n",
       "  'stage': 847,\n",
       "  'marriag': 848,\n",
       "  'straight': 849,\n",
       "  'benign': 850,\n",
       "  'master': 851,\n",
       "  'delici': 852,\n",
       "  'design': 853,\n",
       "  'sin': 854,\n",
       "  'argu': 855,\n",
       "  'nocturn': 856,\n",
       "  'terror': 857,\n",
       "  'skirt': 858,\n",
       "  'tini': 859,\n",
       "  'score': 860,\n",
       "  'hurrah': 861,\n",
       "  'snow-wreath': 862,\n",
       "  'split': 863,\n",
       "  'larder': 864,\n",
       "  'push': 865,\n",
       "  'it.': 866,\n",
       "  'brown': 867,\n",
       "  'exultingli': 868,\n",
       "  'black': 869,\n",
       "  'heroin': 870,\n",
       "  'fullest': 871,\n",
       "  'wildest': 872,\n",
       "  'confound': 873,\n",
       "  'assist': 874,\n",
       "  'east': 875,\n",
       "  'wors': 876,\n",
       "  'know': 877,\n",
       "  'shorter': 878,\n",
       "  'rosenburg': 879,\n",
       "  'mere': 880,\n",
       "  'painter': 881,\n",
       "  'sens': 882,\n",
       "  'spinning-wheel': 883,\n",
       "  'majesti': 884,\n",
       "  'pace': 885,\n",
       "  'outward': 886,\n",
       "  'island': 887,\n",
       "  'compani': 888,\n",
       "  'qualiti': 889,\n",
       "  'half-nak': 890,\n",
       "  'melt': 891,\n",
       "  'spent': 892,\n",
       "  'sup': 893,\n",
       "  'quickli': 894,\n",
       "  'bodi': 895,\n",
       "  'neighborhood': 896,\n",
       "  'confus': 897,\n",
       "  'warm': 898,\n",
       "  'knee': 899,\n",
       "  'wilt': 900,\n",
       "  'thirsti': 901,\n",
       "  'pat': 902,\n",
       "  'resolut': 903,\n",
       "  'grand': 904,\n",
       "  'chaotic': 905,\n",
       "  'beforehand': 906,\n",
       "  'enabl': 907,\n",
       "  'pave': 908,\n",
       "  'pussy-cat': 909,\n",
       "  'absolut': 910,\n",
       "  'imposs': 911,\n",
       "  'narrat': 912,\n",
       "  'dust-box': 913,\n",
       "  'whither': 914,\n",
       "  'profus': 915,\n",
       "  'geometr': 916,\n",
       "  'smithi': 917,\n",
       "  'drink': 918,\n",
       "  'vision': 919,\n",
       "  'kay': 920,\n",
       "  'leant': 921,\n",
       "  'hair': 922,\n",
       "  'tingl': 923,\n",
       "  'thing': 924,\n",
       "  'saloon': 925,\n",
       "  'c.': 926,\n",
       "  'griev': 927,\n",
       "  'destini': 928,\n",
       "  'foolish': 929,\n",
       "  'coral': 930,\n",
       "  'turn-up': 931,\n",
       "  'blow': 932,\n",
       "  'instrument': 933,\n",
       "  'wind': 934,\n",
       "  'see': 935,\n",
       "  'theologia': 936,\n",
       "  'threw': 937,\n",
       "  'ala': 938,\n",
       "  'everyon': 939,\n",
       "  'scissor': 940,\n",
       "  'ornament': 941,\n",
       "  'despit': 942,\n",
       "  'piano': 943,\n",
       "  'field-flow': 944,\n",
       "  'shine': 945,\n",
       "  'concern': 946,\n",
       "  'utter': 947,\n",
       "  'veget': 948,\n",
       "  'parent': 949,\n",
       "  'whenc': 950,\n",
       "  'bitter': 951,\n",
       "  'yellow': 952,\n",
       "  'allow': 953,\n",
       "  'scarc': 954,\n",
       "  'hauch': 955,\n",
       "  'discov': 956,\n",
       "  'forest': 957,\n",
       "  'pavement': 958,\n",
       "  'enorm': 959,\n",
       "  'wick': 960,\n",
       "  'bodili': 961,\n",
       "  'eldest': 962,\n",
       "  'organ': 963,\n",
       "  'ill.': 964,\n",
       "  'cleric': 965,\n",
       "  'pleasur': 966,\n",
       "  'wipe': 967,\n",
       "  'alp': 968,\n",
       "  'awaken': 969,\n",
       "  'anew': 970,\n",
       "  'flower-garden': 971,\n",
       "  'cast': 972,\n",
       "  'tour': 973,\n",
       "  'thou': 974,\n",
       "  'sine': 975,\n",
       "  'house-dog': 976,\n",
       "  'peac': 977,\n",
       "  'orang': 978,\n",
       "  'whimper': 979,\n",
       "  'horn': 980,\n",
       "  'eas': 981,\n",
       "  'prattl': 982,\n",
       "  'postman': 983,\n",
       "  'distanc': 984,\n",
       "  'heart-ach': 985,\n",
       "  'charact': 986,\n",
       "  'stop': 987,\n",
       "  'support': 988,\n",
       "  'rever': 989,\n",
       "  'fall': 990,\n",
       "  'respir': 991,\n",
       "  'desper': 992,\n",
       "  'upper': 993,\n",
       "  'fuss': 994,\n",
       "  'despair.': 995,\n",
       "  'verili': 996,\n",
       "  'sleeper': 997,\n",
       "  'advis': 998,\n",
       "  'bark': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355a1a",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM Masked Language Modeling\n",
    "\n",
    "references: \n",
    "\n",
    "https://keras.io/examples/nlp/masked_language_modeling/#create-bert-model-pretraining-model-for-masked-language-modeling\n",
    "\n",
    "https://www.kaggle.com/code/ritvik1909/masked-language-modelling-rnn#Data-Preparation\n",
    "\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "questions:\n",
    "- should we split data by sentence instead of by fixed window size of 20?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51007c",
   "metadata": {},
   "source": [
    "### more data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to vectors\n",
    "vectorized_text = list(map(lambda x: vocabulary[x], train_data))\n",
    "vectorized_text = np.array(vectorized_text)\n",
    "\n",
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "vocabulary['[mask]'] = mask_id\n",
    "\n",
    "# split data into sequences of length 20\n",
    "vectorized_text_len = len(vectorized_text) - (len(vectorized_text) % 20)\n",
    "vectorized_text = vectorized_text[:vectorized_text_len]\n",
    "vectorized_text = np.reshape(vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34a2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1208,  799, 1343, ..., 1920, 3457, 3627],\n",
       "       [2511, 1293, 1574, ...,  458, 2797, 1940],\n",
       "       [1920, 1343,  709, ..., 1586, 1293,  583],\n",
       "       ...,\n",
       "       [3906, 2324,  799, ...,    7,  756, 1586],\n",
       "       [3742,    7,  756, ..., 2817, 2624,  641],\n",
       "       [3628, 3563, 3906, ..., 3701, 3102, 1178]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f04468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_one_input_label(sequence):\n",
    "    \n",
    "    # randomly choose one position in sequence to mask\n",
    "    mask = np.random.randint(low=0, high=20)\n",
    "    \n",
    "    # add mask to input\n",
    "    masked_input = [token if i != mask else mask_id for i, token in enumerate(sequence)]\n",
    "    \n",
    "    # set all values in label to -1(ignored by loss function) except the value at the masked position\n",
    "    label = [-1 if i!= mask else token for i, token in enumerate(sequence)]\n",
    "    return masked_input, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fbd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get masked inputs and labels\n",
    "def get_masked_inputs_labels(text):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for seq in text:\n",
    "        x,y = mask_one_input_label(seq)\n",
    "        inputs.append(x)\n",
    "        labels.append(y)\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8dcf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = get_masked_inputs_labels(vectorized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a73b",
   "metadata": {},
   "source": [
    "### bi-directional lstm model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9830fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masked language modeling class\n",
    "class LSTM_MLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, input_length):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "        : param vocab_size : The number of unique words in the data\n",
    "        : param hidden_size   : The size of your desired RNN\n",
    "        : param embed_size : The size of your latent embedding\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_length = input_length\n",
    "\n",
    "        ## TODO: define your trainable variables and/or layers here. This should include an\n",
    "        ## embedding component, and any other variables/layers you require.\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size+1, output_dim=self.embed_size)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "        self.dense1 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
    "\n",
    "        # fully connected linear layers\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        You must use an embedding layer as the first layer of your network (i.e. tf.nn.embedding_lookup or tf.keras.layers.Embedding)\n",
    "        :param inputs: word ids of shape (batch_size, 2)\n",
    "        :return: logits: The batch element probabilities as a tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # embedding layer\n",
    "        x = inputs\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9c274c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 14s 72ms/step - loss: 7.0499\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 7s 74ms/step - loss: 5.6662\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 5.3911\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 5.2313\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 7s 73ms/step - loss: 5.0641\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 7s 69ms/step - loss: 4.8635\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 4.6110\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 4.3779\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 6s 60ms/step - loss: 4.1853\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.9469\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 3.7062\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.4725\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 3.2321\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 6s 62ms/step - loss: 3.0094\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 2.8030\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 2.5666\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 6s 65ms/step - loss: 2.3164\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 6s 64ms/step - loss: 2.0705\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 7s 66ms/step - loss: 1.8239\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 1.5914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8abf84fdf0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_MLM(vocab_size, 64, 20)\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "# accuracy is not a good measure\n",
    "model.compile(loss=loss_metric, optimizer='adam')\n",
    "model.fit(x=inputs, y=labels, batch_size=20, epochs=20) \n",
    "# we do not need validation because our purpose is only to learn the patterns in our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4294ef",
   "metadata": {},
   "source": [
    "### get predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ca807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction: still in progress\n",
    "\n",
    "def get_predicted_probability(masked_sentence, target_word):\n",
    "    masked_sentence = masked_sentence.split(' ')\n",
    "    mask_loc = masked_sentence.index('[mask]')\n",
    "    target_id = vocabulary[target_word]\n",
    "    query_id = [vocabulary[q] for q in masked_sentence]\n",
    "\n",
    "    query_id = tf.expand_dims(query_id, axis=0)\n",
    "    pred = model(query_id)[:,mask_loc, target_id]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd046776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '[mask] like beauti dress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b1eeb29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02857665], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "934b9680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01522405], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "305dd1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00066057], dtype=float32)>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dfe65af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.19514e-05], dtype=float32)>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "556080d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'evil old [mask]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cb73c75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.3034821e-05], dtype=float32)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e27cd75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.0170372e-05], dtype=float32)>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "469602ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.3883402e-05], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'pretti [mask]'\n",
    "get_predicted_probability(test_sentence, 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "11a696da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00011371], dtype=float32)>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability(test_sentence, 'boy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64fc2d",
   "metadata": {},
   "source": [
    "### access embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0ef8cd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4127, 64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed693a",
   "metadata": {},
   "source": [
    "### testing lstm model on HW4 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f9ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/hw4_train.txt', \"r\")\n",
    "\n",
    "hw4_data = file.read()\n",
    "hw4_data = hw4_data.replace('\\n', ' ').split(' ')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c71b8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_vocabulary, hw4_vocab_size = get_vocab(hw4_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b88121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to vectors\n",
    "hw4_vectorized_text = list(map(lambda x: hw4_vocabulary[x], hw4_data))\n",
    "hw4_vectorized_text = np.array(hw4_vectorized_text)\n",
    "\n",
    "# add [mask] to vocabulary\n",
    "mask_id = vocab_size\n",
    "hw4_vocabulary['[mask]'] = mask_id\n",
    "\n",
    "# split data into sequences of length 20\n",
    "hw4_vectorized_text_len = len(hw4_vectorized_text) - (len(hw4_vectorized_text) % 20)\n",
    "hw4_vectorized_text = hw4_vectorized_text[:hw4_vectorized_text_len]\n",
    "hw4_vectorized_text = np.reshape(hw4_vectorized_text,[-1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d5989da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_inputs, hw4_labels = get_masked_inputs_labels(hw4_vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0e7a53e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing model performance on hw4 data:\n",
    "# model = LSTM_MLM(hw4_vocab_size, 64, 20)\n",
    "# loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "# model.compile(loss=loss_metric, optimizer='adam')\n",
    "# model.fit(x=hw4_inputs, y=hw4_labels, batch_size=20, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696ff06",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aa8ac",
   "metadata": {},
   "source": [
    "references: \"Attention Is All You Need\" paper by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e923d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.query = tf.keras.layers.Dense(d_model)\n",
    "        self.key = tf.keras.layers.Dense(d_model)\n",
    "        self.value = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "        \n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32f12b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.att = SingleHeadAttention(d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model * 4, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, _ = self.att(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22cb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_MLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_size, input_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size+1, output_dim=self.embed_size)\n",
    "        self.transformer_block = TransformerBlock(self.embed_size)\n",
    "        self.dense1 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_block(x, training=True)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64138dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 6s 37ms/step - loss: 7.6252\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.9200\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.3986\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.7189\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8785\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0417\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.2777\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 3s 34ms/step - loss: 1.6189\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.1057\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.7190\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.4373\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 4s 41ms/step - loss: 0.2575\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 0.1547\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 0.1017\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 0.0725\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 0.0548\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.0447\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 0.0361\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.0296\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.0262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13cf46f50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t = Transformer_MLM(vocab_size, 64, 20)\n",
    "loss_metric = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=-1)\n",
    "model_t.compile(loss=loss_metric, optimizer='adam')\n",
    "model_t.fit(x=inputs, y=labels, batch_size=20, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5bcb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probability_transformer(masked_sentence, target_word):\n",
    "    masked_sentence = masked_sentence.split(' ')\n",
    "    mask_loc = masked_sentence.index('[mask]')\n",
    "    target_id = vocabulary[target_word]\n",
    "    query_id = [vocabulary[q] for q in masked_sentence]\n",
    "\n",
    "    query_id = tf.expand_dims(query_id, axis=0)\n",
    "    pred = model_t(query_id)[:,mask_loc, target_id]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "050658fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '[mask] like beauti dress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f62b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.3778994e-05], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability_transformer(test_sentence, 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fabc4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00123803], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability_transformer(test_sentence, 'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbcd1d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01000271], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability_transformer(test_sentence, 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a2f92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([9.1098386e-08], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_probability_transformer(test_sentence, 'king')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
